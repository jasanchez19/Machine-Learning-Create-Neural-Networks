{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOO1teS/Z72jvJV69MjG8X4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6d3dbe2c78e94a35835c29ff5e5472f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bcb9a258475b49809a51060b81c805f4",
              "IPY_MODEL_c705a30743dc494e804367f99caaaa40",
              "IPY_MODEL_80cdef3b448f4e8f8d5e75399f69ef25"
            ],
            "layout": "IPY_MODEL_12a9ff7734da47d3abbd6cc93d68121f"
          }
        },
        "bcb9a258475b49809a51060b81c805f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46d18dfb3dd140f0be78d6af34650a4e",
            "placeholder": "​",
            "style": "IPY_MODEL_dc5b87d94bea41aebfd8436297e17af5",
            "value": "100%"
          }
        },
        "c705a30743dc494e804367f99caaaa40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8771663975174076b093e0e15ec3d621",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_afc6d30241d54256aa8608bd355f3b3a",
            "value": 170498071
          }
        },
        "80cdef3b448f4e8f8d5e75399f69ef25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf637f44aacf48e7a80e51e3172f0757",
            "placeholder": "​",
            "style": "IPY_MODEL_f269baf11ddc4d90964f673c58bcf96a",
            "value": " 170498071/170498071 [00:13&lt;00:00, 15651568.85it/s]"
          }
        },
        "12a9ff7734da47d3abbd6cc93d68121f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46d18dfb3dd140f0be78d6af34650a4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc5b87d94bea41aebfd8436297e17af5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8771663975174076b093e0e15ec3d621": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afc6d30241d54256aa8608bd355f3b3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bf637f44aacf48e7a80e51e3172f0757": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f269baf11ddc4d90964f673c58bcf96a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jasanchez19/Machine-Learning-Create-Neural-Networks/blob/main/Homework6JorgeSanchezECGR4105.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "torch.set_printoptions(edgeitems=2)\n",
        "torch.manual_seed(123)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eojl0XDV_fpy",
        "outputId": "8cfe45a2-847b-4e08-b129-b93491585a56"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f6a16d2cfb0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 1"
      ],
      "metadata": {
        "id": "9afoWt-E_bhA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part A"
      ],
      "metadata": {
        "id": "fH82LN-zE8QT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xawSH37n_qR7",
        "outputId": "5f4d8eb4-669b-4668-cea1-0b955497cae2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "data_path = '../data-unversioned/p1ch7/'\n",
        "cifar10 = datasets.CIFAR10(data_path, train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4915, 0.4823, 0.4468),(0.2470, 0.2435, 0.2616))]))\n",
        "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4915, 0.4823, 0.4468),(0.2470, 0.2435, 0.2616))]))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "6d3dbe2c78e94a35835c29ff5e5472f7",
            "bcb9a258475b49809a51060b81c805f4",
            "c705a30743dc494e804367f99caaaa40",
            "80cdef3b448f4e8f8d5e75399f69ef25",
            "12a9ff7734da47d3abbd6cc93d68121f",
            "46d18dfb3dd140f0be78d6af34650a4e",
            "dc5b87d94bea41aebfd8436297e17af5",
            "8771663975174076b093e0e15ec3d621",
            "afc6d30241d54256aa8608bd355f3b3a",
            "bf637f44aacf48e7a80e51e3172f0757",
            "f269baf11ddc4d90964f673c58bcf96a"
          ]
        },
        "id": "lNEFWXOFAvas",
        "outputId": "7dbdfcd7-4f0c-45ff-b526-20dbca1ba628"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data-unversioned/p1ch7/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6d3dbe2c78e94a35835c29ff5e5472f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data-unversioned/p1ch7/cifar-10-python.tar.gz to ../data-unversioned/p1ch7/\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.act1 = nn.Tanh()\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "        self.act2 = nn.Tanh()\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
        "        self.act3 = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.pool1(self.act1(self.conv1(x)))\n",
        "        out = self.pool2(self.act2(self.conv2(out)))\n",
        "        out = out.view(-1, 8 * 8 * 8) # <1>\n",
        "        out = self.act3(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "fjumD4CRA4rd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    loss_train = 0.0\n",
        "    for imgs, labels in train_loader:\n",
        "      imgs = imgs.to(device=device)\n",
        "      labels = labels.to(device=device)\n",
        "      outputs = model(imgs)\n",
        "      loss = loss_fn(outputs, labels)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      loss_train += loss.item()\n",
        "    print('{} Epoch {}, Training loss {}'.format(\n",
        "    datetime.datetime.now(), epoch,\n",
        "    loss_train / len(train_loader)))"
      ],
      "metadata": {
        "id": "3DzcCsaEBF2_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, train_loader, val_loader):\n",
        "  for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "      for imgs, labels in loader:\n",
        "          imgs, labels = imgs.to(device), labels.to(device)\n",
        "          batchsize = imgs.shape[0]\n",
        "          outputs = model(imgs)\n",
        "          _, predicted = torch.max(outputs, dim=1)\n",
        "          total += labels.shape[0]\n",
        "          correct += int((predicted == labels).sum())\n",
        "    print(\"Accuracy {}: {:.2f}\".format(name , correct / total))"
      ],
      "metadata": {
        "id": "OOYSmIccBIjy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                           shuffle=True)  # <1>\n",
        "\n",
        "model = Net().to(device=device)  #  <2>\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)  #  <3>\n",
        "loss_fn = nn.CrossEntropyLoss()  #  <4>\n",
        "\n",
        "training_loop(  # <5>\n",
        "    n_epochs = 300,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esPfLtSWBLlX",
        "outputId": "2b2f3787-f135-4cb6-e4b5-f6fa5eb98b2d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-12-07 16:45:57.743996 Epoch 1, Training loss 2.027217293029551\n",
            "2022-12-07 16:46:10.069563 Epoch 2, Training loss 1.7497720678748987\n",
            "2022-12-07 16:46:22.445291 Epoch 3, Training loss 1.5772512189262664\n",
            "2022-12-07 16:46:34.823357 Epoch 4, Training loss 1.4791934827099675\n",
            "2022-12-07 16:46:47.174110 Epoch 5, Training loss 1.407292333405341\n",
            "2022-12-07 16:46:59.446634 Epoch 6, Training loss 1.3408779095658256\n",
            "2022-12-07 16:47:11.808416 Epoch 7, Training loss 1.281713824214228\n",
            "2022-12-07 16:47:24.160779 Epoch 8, Training loss 1.2311879630436373\n",
            "2022-12-07 16:47:36.550405 Epoch 9, Training loss 1.1906362339053922\n",
            "2022-12-07 16:47:49.062809 Epoch 10, Training loss 1.1571512606442738\n",
            "2022-12-07 16:48:01.599214 Epoch 11, Training loss 1.1288810272503387\n",
            "2022-12-07 16:48:14.085641 Epoch 12, Training loss 1.1048441008686105\n",
            "2022-12-07 16:48:26.641470 Epoch 13, Training loss 1.0834906156867972\n",
            "2022-12-07 16:48:39.093302 Epoch 14, Training loss 1.0671912920292077\n",
            "2022-12-07 16:48:51.199261 Epoch 15, Training loss 1.0493019902340286\n",
            "2022-12-07 16:49:03.035384 Epoch 16, Training loss 1.0344071009427385\n",
            "2022-12-07 16:49:14.855293 Epoch 17, Training loss 1.0206287347752114\n",
            "2022-12-07 16:49:26.721448 Epoch 18, Training loss 1.0089006649563685\n",
            "2022-12-07 16:49:38.600518 Epoch 19, Training loss 0.9971316542924212\n",
            "2022-12-07 16:49:50.471358 Epoch 20, Training loss 0.9849678983773722\n",
            "2022-12-07 16:50:02.261743 Epoch 21, Training loss 0.9753552936684445\n",
            "2022-12-07 16:50:14.134310 Epoch 22, Training loss 0.9660493097341883\n",
            "2022-12-07 16:50:26.131978 Epoch 23, Training loss 0.9582202788204184\n",
            "2022-12-07 16:50:38.086567 Epoch 24, Training loss 0.950143977580473\n",
            "2022-12-07 16:50:50.021386 Epoch 25, Training loss 0.9404497289139292\n",
            "2022-12-07 16:51:02.081193 Epoch 26, Training loss 0.9335407733612353\n",
            "2022-12-07 16:51:13.978719 Epoch 27, Training loss 0.9268986758063821\n",
            "2022-12-07 16:51:25.741878 Epoch 28, Training loss 0.9177452597166876\n",
            "2022-12-07 16:51:37.514849 Epoch 29, Training loss 0.9120408817935173\n",
            "2022-12-07 16:51:49.392023 Epoch 30, Training loss 0.9046198628899996\n",
            "2022-12-07 16:52:01.184827 Epoch 31, Training loss 0.8995896170816153\n",
            "2022-12-07 16:52:13.019843 Epoch 32, Training loss 0.892481859687649\n",
            "2022-12-07 16:52:24.855641 Epoch 33, Training loss 0.8877355992946478\n",
            "2022-12-07 16:52:36.638688 Epoch 34, Training loss 0.8806002967040557\n",
            "2022-12-07 16:52:48.469248 Epoch 35, Training loss 0.8763036853669549\n",
            "2022-12-07 16:53:00.295747 Epoch 36, Training loss 0.8702676672383648\n",
            "2022-12-07 16:53:12.202558 Epoch 37, Training loss 0.8664914721341999\n",
            "2022-12-07 16:53:24.079689 Epoch 38, Training loss 0.8604358316916029\n",
            "2022-12-07 16:53:35.867276 Epoch 39, Training loss 0.8558091363486122\n",
            "2022-12-07 16:53:47.666090 Epoch 40, Training loss 0.850900906964641\n",
            "2022-12-07 16:53:59.464868 Epoch 41, Training loss 0.8461451350956621\n",
            "2022-12-07 16:54:11.366815 Epoch 42, Training loss 0.8424975670054745\n",
            "2022-12-07 16:54:23.258803 Epoch 43, Training loss 0.8374540251691628\n",
            "2022-12-07 16:54:35.034919 Epoch 44, Training loss 0.8330602578418639\n",
            "2022-12-07 16:54:46.805645 Epoch 45, Training loss 0.8282507592454895\n",
            "2022-12-07 16:54:58.689886 Epoch 46, Training loss 0.8233003688742743\n",
            "2022-12-07 16:55:10.588824 Epoch 47, Training loss 0.8204353675817895\n",
            "2022-12-07 16:55:22.471920 Epoch 48, Training loss 0.8161591947688471\n",
            "2022-12-07 16:55:34.442494 Epoch 49, Training loss 0.8127574822710603\n",
            "2022-12-07 16:55:46.351169 Epoch 50, Training loss 0.8095989810002734\n",
            "2022-12-07 16:55:58.205297 Epoch 51, Training loss 0.8059036389488698\n",
            "2022-12-07 16:56:10.066418 Epoch 52, Training loss 0.802594279846572\n",
            "2022-12-07 16:56:22.039023 Epoch 53, Training loss 0.7983103335818367\n",
            "2022-12-07 16:56:33.841729 Epoch 54, Training loss 0.7961141448039228\n",
            "2022-12-07 16:56:45.644726 Epoch 55, Training loss 0.7927924177759443\n",
            "2022-12-07 16:56:57.482031 Epoch 56, Training loss 0.7867382437066959\n",
            "2022-12-07 16:57:09.330631 Epoch 57, Training loss 0.7838377709050313\n",
            "2022-12-07 16:57:21.178856 Epoch 58, Training loss 0.7815883749967341\n",
            "2022-12-07 16:57:33.138525 Epoch 59, Training loss 0.7783092917002681\n",
            "2022-12-07 16:57:45.063187 Epoch 60, Training loss 0.774746255420358\n",
            "2022-12-07 16:57:56.948900 Epoch 61, Training loss 0.7734005350972075\n",
            "2022-12-07 16:58:08.908798 Epoch 62, Training loss 0.7702734944079538\n",
            "2022-12-07 16:58:20.856282 Epoch 63, Training loss 0.7669592657128869\n",
            "2022-12-07 16:58:32.856394 Epoch 64, Training loss 0.7638285914269249\n",
            "2022-12-07 16:58:44.727046 Epoch 65, Training loss 0.7619281194322859\n",
            "2022-12-07 16:58:56.738443 Epoch 66, Training loss 0.7586233030118601\n",
            "2022-12-07 16:59:08.716063 Epoch 67, Training loss 0.7566980411253317\n",
            "2022-12-07 16:59:20.621636 Epoch 68, Training loss 0.753036451080571\n",
            "2022-12-07 16:59:32.623946 Epoch 69, Training loss 0.7513822070549211\n",
            "2022-12-07 16:59:44.685258 Epoch 70, Training loss 0.7476573717563658\n",
            "2022-12-07 16:59:56.642748 Epoch 71, Training loss 0.748178448632855\n",
            "2022-12-07 17:00:08.597113 Epoch 72, Training loss 0.7432403906303293\n",
            "2022-12-07 17:00:20.570471 Epoch 73, Training loss 0.7399454318425235\n",
            "2022-12-07 17:00:32.559647 Epoch 74, Training loss 0.7388931602773154\n",
            "2022-12-07 17:00:44.521458 Epoch 75, Training loss 0.7346129691433114\n",
            "2022-12-07 17:00:56.457363 Epoch 76, Training loss 0.7349712988909554\n",
            "2022-12-07 17:01:08.398744 Epoch 77, Training loss 0.7308793385010546\n",
            "2022-12-07 17:01:20.323438 Epoch 78, Training loss 0.7286658315631129\n",
            "2022-12-07 17:01:32.242192 Epoch 79, Training loss 0.726634863163809\n",
            "2022-12-07 17:01:44.212980 Epoch 80, Training loss 0.7257802011183155\n",
            "2022-12-07 17:01:56.131984 Epoch 81, Training loss 0.7214103060991258\n",
            "2022-12-07 17:02:08.120105 Epoch 82, Training loss 0.7185224847644186\n",
            "2022-12-07 17:02:20.144237 Epoch 83, Training loss 0.717669107877385\n",
            "2022-12-07 17:02:32.182853 Epoch 84, Training loss 0.7146402992632078\n",
            "2022-12-07 17:02:44.197054 Epoch 85, Training loss 0.713300699963594\n",
            "2022-12-07 17:02:56.216018 Epoch 86, Training loss 0.7113317174984671\n",
            "2022-12-07 17:03:08.242531 Epoch 87, Training loss 0.708765861666416\n",
            "2022-12-07 17:03:20.288395 Epoch 88, Training loss 0.7045572859323238\n",
            "2022-12-07 17:03:32.340919 Epoch 89, Training loss 0.7072132738578655\n",
            "2022-12-07 17:03:44.537992 Epoch 90, Training loss 0.7057286138882113\n",
            "2022-12-07 17:03:56.718952 Epoch 91, Training loss 0.7011712254465693\n",
            "2022-12-07 17:04:08.988113 Epoch 92, Training loss 0.7003167517426069\n",
            "2022-12-07 17:04:21.294408 Epoch 93, Training loss 0.6986871112871658\n",
            "2022-12-07 17:04:33.420356 Epoch 94, Training loss 0.695290096389973\n",
            "2022-12-07 17:04:45.560449 Epoch 95, Training loss 0.694945563097744\n",
            "2022-12-07 17:04:57.690267 Epoch 96, Training loss 0.6911070259178386\n",
            "2022-12-07 17:05:09.705631 Epoch 97, Training loss 0.6922916370584532\n",
            "2022-12-07 17:05:21.722085 Epoch 98, Training loss 0.6887438436755744\n",
            "2022-12-07 17:05:33.777154 Epoch 99, Training loss 0.6866435054546732\n",
            "2022-12-07 17:05:45.812299 Epoch 100, Training loss 0.6839710695435629\n",
            "2022-12-07 17:05:57.822701 Epoch 101, Training loss 0.6825941989717581\n",
            "2022-12-07 17:06:09.911254 Epoch 102, Training loss 0.6836636915917287\n",
            "2022-12-07 17:06:21.980819 Epoch 103, Training loss 0.6796613828757839\n",
            "2022-12-07 17:06:34.098813 Epoch 104, Training loss 0.6784877778242921\n",
            "2022-12-07 17:06:46.199643 Epoch 105, Training loss 0.6764923410342477\n",
            "2022-12-07 17:06:58.211690 Epoch 106, Training loss 0.6770906388912055\n",
            "2022-12-07 17:07:10.203469 Epoch 107, Training loss 0.6727477848682257\n",
            "2022-12-07 17:07:22.203204 Epoch 108, Training loss 0.6729451946895141\n",
            "2022-12-07 17:07:34.111050 Epoch 109, Training loss 0.6702689986171015\n",
            "2022-12-07 17:07:46.101189 Epoch 110, Training loss 0.669837480539556\n",
            "2022-12-07 17:07:58.101471 Epoch 111, Training loss 0.6672753552570367\n",
            "2022-12-07 17:08:10.096376 Epoch 112, Training loss 0.666141594119389\n",
            "2022-12-07 17:08:22.077331 Epoch 113, Training loss 0.6635896113065197\n",
            "2022-12-07 17:08:34.076752 Epoch 114, Training loss 0.6625400999241777\n",
            "2022-12-07 17:08:46.051341 Epoch 115, Training loss 0.6612662996934808\n",
            "2022-12-07 17:08:58.057127 Epoch 116, Training loss 0.6609440285455236\n",
            "2022-12-07 17:09:10.105647 Epoch 117, Training loss 0.6584971343998409\n",
            "2022-12-07 17:09:22.180740 Epoch 118, Training loss 0.6577630756837328\n",
            "2022-12-07 17:09:34.184986 Epoch 119, Training loss 0.6551853052871611\n",
            "2022-12-07 17:09:46.170812 Epoch 120, Training loss 0.654509891824954\n",
            "2022-12-07 17:09:58.135558 Epoch 121, Training loss 0.6521554216551964\n",
            "2022-12-07 17:10:10.084574 Epoch 122, Training loss 0.6518405526495346\n",
            "2022-12-07 17:10:22.097662 Epoch 123, Training loss 0.6496748844223559\n",
            "2022-12-07 17:10:34.057072 Epoch 124, Training loss 0.6484294082883679\n",
            "2022-12-07 17:10:45.994648 Epoch 125, Training loss 0.6461867884068233\n",
            "2022-12-07 17:10:57.935460 Epoch 126, Training loss 0.644850091556149\n",
            "2022-12-07 17:11:10.116830 Epoch 127, Training loss 0.6447989832028709\n",
            "2022-12-07 17:11:22.369123 Epoch 128, Training loss 0.6431509802484756\n",
            "2022-12-07 17:11:34.518489 Epoch 129, Training loss 0.6407849789046876\n",
            "2022-12-07 17:11:46.799852 Epoch 130, Training loss 0.6395820680710361\n",
            "2022-12-07 17:11:58.997722 Epoch 131, Training loss 0.6383457559606304\n",
            "2022-12-07 17:12:11.037621 Epoch 132, Training loss 0.6371049623355232\n",
            "2022-12-07 17:12:22.944249 Epoch 133, Training loss 0.6361963881174927\n",
            "2022-12-07 17:12:34.914011 Epoch 134, Training loss 0.6346260819898542\n",
            "2022-12-07 17:12:46.833991 Epoch 135, Training loss 0.633506213650679\n",
            "2022-12-07 17:12:58.923924 Epoch 136, Training loss 0.6316717266655334\n",
            "2022-12-07 17:13:11.008653 Epoch 137, Training loss 0.632120301160971\n",
            "2022-12-07 17:13:23.000337 Epoch 138, Training loss 0.628563175535263\n",
            "2022-12-07 17:13:35.022663 Epoch 139, Training loss 0.6297589160139908\n",
            "2022-12-07 17:13:46.977083 Epoch 140, Training loss 0.6266753918603253\n",
            "2022-12-07 17:13:58.864949 Epoch 141, Training loss 0.6260785766879616\n",
            "2022-12-07 17:14:10.895723 Epoch 142, Training loss 0.6244005322303918\n",
            "2022-12-07 17:14:22.920551 Epoch 143, Training loss 0.6248906616435941\n",
            "2022-12-07 17:14:34.956704 Epoch 144, Training loss 0.6210784049671324\n",
            "2022-12-07 17:14:46.906749 Epoch 145, Training loss 0.6213148972186286\n",
            "2022-12-07 17:14:58.812455 Epoch 146, Training loss 0.6211824247904141\n",
            "2022-12-07 17:15:10.845436 Epoch 147, Training loss 0.6193713019113711\n",
            "2022-12-07 17:15:22.835431 Epoch 148, Training loss 0.618071615657843\n",
            "2022-12-07 17:15:34.821714 Epoch 149, Training loss 0.6161159958375995\n",
            "2022-12-07 17:15:46.877020 Epoch 150, Training loss 0.616476524554555\n",
            "2022-12-07 17:15:59.083431 Epoch 151, Training loss 0.6151174437588133\n",
            "2022-12-07 17:16:11.026623 Epoch 152, Training loss 0.6128982332204004\n",
            "2022-12-07 17:16:23.001905 Epoch 153, Training loss 0.6133198992294424\n",
            "2022-12-07 17:16:34.881940 Epoch 154, Training loss 0.6116094247002126\n",
            "2022-12-07 17:16:46.699550 Epoch 155, Training loss 0.6092663276607119\n",
            "2022-12-07 17:16:58.540880 Epoch 156, Training loss 0.607936721018818\n",
            "2022-12-07 17:17:10.334440 Epoch 157, Training loss 0.6066780310610066\n",
            "2022-12-07 17:17:22.495485 Epoch 158, Training loss 0.6069676342522702\n",
            "2022-12-07 17:17:34.525006 Epoch 159, Training loss 0.606150957743835\n",
            "2022-12-07 17:17:46.485677 Epoch 160, Training loss 0.6034934324834048\n",
            "2022-12-07 17:17:58.366365 Epoch 161, Training loss 0.6026644404891812\n",
            "2022-12-07 17:18:10.198543 Epoch 162, Training loss 0.6011938329433542\n",
            "2022-12-07 17:18:22.045898 Epoch 163, Training loss 0.6010830663430416\n",
            "2022-12-07 17:18:33.825687 Epoch 164, Training loss 0.600764503145157\n",
            "2022-12-07 17:18:45.676187 Epoch 165, Training loss 0.6002299167844646\n",
            "2022-12-07 17:18:57.502020 Epoch 166, Training loss 0.5989159636409082\n",
            "2022-12-07 17:19:09.289518 Epoch 167, Training loss 0.5977038378110322\n",
            "2022-12-07 17:19:21.112327 Epoch 168, Training loss 0.5960926978903658\n",
            "2022-12-07 17:19:32.922439 Epoch 169, Training loss 0.5956404617299205\n",
            "2022-12-07 17:19:44.779435 Epoch 170, Training loss 0.593697770782139\n",
            "2022-12-07 17:19:56.648869 Epoch 171, Training loss 0.5944138764953979\n",
            "2022-12-07 17:20:08.516070 Epoch 172, Training loss 0.592272339155302\n",
            "2022-12-07 17:20:20.382330 Epoch 173, Training loss 0.591830189423183\n",
            "2022-12-07 17:20:32.234433 Epoch 174, Training loss 0.5901062996186259\n",
            "2022-12-07 17:20:44.029770 Epoch 175, Training loss 0.5891646734055351\n",
            "2022-12-07 17:20:55.819287 Epoch 176, Training loss 0.5889149024663374\n",
            "2022-12-07 17:21:07.631677 Epoch 177, Training loss 0.5871357459317693\n",
            "2022-12-07 17:21:19.490679 Epoch 178, Training loss 0.58812003336904\n",
            "2022-12-07 17:21:31.406058 Epoch 179, Training loss 0.586620319682314\n",
            "2022-12-07 17:21:43.304212 Epoch 180, Training loss 0.5865440708215889\n",
            "2022-12-07 17:21:55.170810 Epoch 181, Training loss 0.5853012093269002\n",
            "2022-12-07 17:22:07.051575 Epoch 182, Training loss 0.5829727532689833\n",
            "2022-12-07 17:22:18.979251 Epoch 183, Training loss 0.5829142380858321\n",
            "2022-12-07 17:22:30.872010 Epoch 184, Training loss 0.5820797986691565\n",
            "2022-12-07 17:22:42.683042 Epoch 185, Training loss 0.5813749539272864\n",
            "2022-12-07 17:22:54.460370 Epoch 186, Training loss 0.5812050607198339\n",
            "2022-12-07 17:23:06.242736 Epoch 187, Training loss 0.5767935901651602\n",
            "2022-12-07 17:23:18.075519 Epoch 188, Training loss 0.5778589051626527\n",
            "2022-12-07 17:23:29.950502 Epoch 189, Training loss 0.5773905418108186\n",
            "2022-12-07 17:23:41.811053 Epoch 190, Training loss 0.5770492842587669\n",
            "2022-12-07 17:23:53.654624 Epoch 191, Training loss 0.575688197236994\n",
            "2022-12-07 17:24:05.519851 Epoch 192, Training loss 0.5737381748797948\n",
            "2022-12-07 17:24:17.414930 Epoch 193, Training loss 0.5750257414777565\n",
            "2022-12-07 17:24:29.236883 Epoch 194, Training loss 0.5734646217826077\n",
            "2022-12-07 17:24:41.048757 Epoch 195, Training loss 0.5739839298036092\n",
            "2022-12-07 17:24:52.900966 Epoch 196, Training loss 0.5714327953279476\n",
            "2022-12-07 17:25:04.678256 Epoch 197, Training loss 0.5722066486049491\n",
            "2022-12-07 17:25:16.573916 Epoch 198, Training loss 0.5706154063648885\n",
            "2022-12-07 17:25:28.459096 Epoch 199, Training loss 0.5699315156663776\n",
            "2022-12-07 17:25:40.317820 Epoch 200, Training loss 0.5674335894453556\n",
            "2022-12-07 17:25:52.140995 Epoch 201, Training loss 0.5677674294204054\n",
            "2022-12-07 17:26:03.929241 Epoch 202, Training loss 0.5665519304973695\n",
            "2022-12-07 17:26:15.811401 Epoch 203, Training loss 0.5648813045695614\n",
            "2022-12-07 17:26:27.608688 Epoch 204, Training loss 0.566158907950077\n",
            "2022-12-07 17:26:39.419561 Epoch 205, Training loss 0.5640487468151181\n",
            "2022-12-07 17:26:51.289344 Epoch 206, Training loss 0.5645302302963898\n",
            "2022-12-07 17:27:03.218989 Epoch 207, Training loss 0.5640062224453367\n",
            "2022-12-07 17:27:15.143890 Epoch 208, Training loss 0.563120207022828\n",
            "2022-12-07 17:27:27.105371 Epoch 209, Training loss 0.5631192040527263\n",
            "2022-12-07 17:27:39.078856 Epoch 210, Training loss 0.5610587045245463\n",
            "2022-12-07 17:27:51.101133 Epoch 211, Training loss 0.5607718017781177\n",
            "2022-12-07 17:28:02.856119 Epoch 212, Training loss 0.5588348426706041\n",
            "2022-12-07 17:28:14.721700 Epoch 213, Training loss 0.560025250362923\n",
            "2022-12-07 17:28:26.544023 Epoch 214, Training loss 0.5592681591010764\n",
            "2022-12-07 17:28:38.416824 Epoch 215, Training loss 0.5605037772213407\n",
            "2022-12-07 17:28:50.227464 Epoch 216, Training loss 0.5566534861884154\n",
            "2022-12-07 17:29:02.075672 Epoch 217, Training loss 0.5562012186440666\n",
            "2022-12-07 17:29:13.940792 Epoch 218, Training loss 0.556371031819707\n",
            "2022-12-07 17:29:25.796024 Epoch 219, Training loss 0.5562182905347756\n",
            "2022-12-07 17:29:37.616255 Epoch 220, Training loss 0.5558420041637957\n",
            "2022-12-07 17:29:49.483288 Epoch 221, Training loss 0.5544188420104859\n",
            "2022-12-07 17:30:01.338980 Epoch 222, Training loss 0.5529012434622821\n",
            "2022-12-07 17:30:13.228043 Epoch 223, Training loss 0.5542742307000148\n",
            "2022-12-07 17:30:25.188708 Epoch 224, Training loss 0.5512514351807591\n",
            "2022-12-07 17:30:37.033398 Epoch 225, Training loss 0.5507144206548895\n",
            "2022-12-07 17:30:48.987606 Epoch 226, Training loss 0.5538924318521529\n",
            "2022-12-07 17:31:00.903596 Epoch 227, Training loss 0.5509720983560128\n",
            "2022-12-07 17:31:12.753629 Epoch 228, Training loss 0.5509617077115246\n",
            "2022-12-07 17:31:24.596394 Epoch 229, Training loss 0.5492747187461999\n",
            "2022-12-07 17:31:36.445215 Epoch 230, Training loss 0.5491362479336731\n",
            "2022-12-07 17:31:48.355217 Epoch 231, Training loss 0.5487360929894021\n",
            "2022-12-07 17:32:00.188257 Epoch 232, Training loss 0.5473323127878901\n",
            "2022-12-07 17:32:12.021631 Epoch 233, Training loss 0.5469313411380324\n",
            "2022-12-07 17:32:23.948415 Epoch 234, Training loss 0.5481624584978498\n",
            "2022-12-07 17:32:35.996285 Epoch 235, Training loss 0.5456382126911826\n",
            "2022-12-07 17:32:47.946517 Epoch 236, Training loss 0.5449268953955692\n",
            "2022-12-07 17:32:59.894061 Epoch 237, Training loss 0.543315798768302\n",
            "2022-12-07 17:33:11.809840 Epoch 238, Training loss 0.5433025268642494\n",
            "2022-12-07 17:33:23.665327 Epoch 239, Training loss 0.5421191019856412\n",
            "2022-12-07 17:33:35.475739 Epoch 240, Training loss 0.5426742617431504\n",
            "2022-12-07 17:33:47.366819 Epoch 241, Training loss 0.5395212627928275\n",
            "2022-12-07 17:33:59.219320 Epoch 242, Training loss 0.5416064102707616\n",
            "2022-12-07 17:34:11.072040 Epoch 243, Training loss 0.5404902555982171\n",
            "2022-12-07 17:34:23.011659 Epoch 244, Training loss 0.5400763953395207\n",
            "2022-12-07 17:34:34.870348 Epoch 245, Training loss 0.540352707552483\n",
            "2022-12-07 17:34:46.673649 Epoch 246, Training loss 0.5418865222988836\n",
            "2022-12-07 17:34:58.486294 Epoch 247, Training loss 0.5388902977604391\n",
            "2022-12-07 17:35:10.328484 Epoch 248, Training loss 0.5397573528082474\n",
            "2022-12-07 17:35:22.168035 Epoch 249, Training loss 0.5405311467473769\n",
            "2022-12-07 17:35:34.014597 Epoch 250, Training loss 0.5386777555622408\n",
            "2022-12-07 17:35:45.920959 Epoch 251, Training loss 0.5370683644509986\n",
            "2022-12-07 17:35:57.808723 Epoch 252, Training loss 0.5363697997291984\n",
            "2022-12-07 17:36:09.754679 Epoch 253, Training loss 0.5344025159202268\n",
            "2022-12-07 17:36:21.683423 Epoch 254, Training loss 0.5349668224373132\n",
            "2022-12-07 17:36:33.586979 Epoch 255, Training loss 0.5340999823320857\n",
            "2022-12-07 17:36:45.556485 Epoch 256, Training loss 0.5349968571949493\n",
            "2022-12-07 17:36:57.541405 Epoch 257, Training loss 0.5346017027145151\n",
            "2022-12-07 17:37:09.454257 Epoch 258, Training loss 0.5335741133412437\n",
            "2022-12-07 17:37:21.296245 Epoch 259, Training loss 0.5326777214894209\n",
            "2022-12-07 17:37:33.187131 Epoch 260, Training loss 0.5309005258485789\n",
            "2022-12-07 17:37:45.341665 Epoch 261, Training loss 0.5320412465335463\n",
            "2022-12-07 17:37:57.354810 Epoch 262, Training loss 0.5303367344695894\n",
            "2022-12-07 17:38:09.337303 Epoch 263, Training loss 0.5315780204808925\n",
            "2022-12-07 17:38:21.395826 Epoch 264, Training loss 0.5291063448657161\n",
            "2022-12-07 17:38:33.320748 Epoch 265, Training loss 0.529540461538088\n",
            "2022-12-07 17:38:45.313468 Epoch 266, Training loss 0.529064921104847\n",
            "2022-12-07 17:38:57.236178 Epoch 267, Training loss 0.5318961438468045\n",
            "2022-12-07 17:39:09.212012 Epoch 268, Training loss 0.5297215816271884\n",
            "2022-12-07 17:39:21.221165 Epoch 269, Training loss 0.5277354325860968\n",
            "2022-12-07 17:39:33.114405 Epoch 270, Training loss 0.5291353477084119\n",
            "2022-12-07 17:39:45.093578 Epoch 271, Training loss 0.5277708619451889\n",
            "2022-12-07 17:39:56.966629 Epoch 272, Training loss 0.5264251204707738\n",
            "2022-12-07 17:40:08.806412 Epoch 273, Training loss 0.5269296806867775\n",
            "2022-12-07 17:40:20.877669 Epoch 274, Training loss 0.5268591726222611\n",
            "2022-12-07 17:40:32.842679 Epoch 275, Training loss 0.523758505159022\n",
            "2022-12-07 17:40:44.698468 Epoch 276, Training loss 0.5246718767887492\n",
            "2022-12-07 17:40:56.704054 Epoch 277, Training loss 0.5251339308707915\n",
            "2022-12-07 17:41:08.604376 Epoch 278, Training loss 0.5251793197696776\n",
            "2022-12-07 17:41:20.584486 Epoch 279, Training loss 0.5245264254300795\n",
            "2022-12-07 17:41:32.511262 Epoch 280, Training loss 0.5236137411783418\n",
            "2022-12-07 17:41:44.368996 Epoch 281, Training loss 0.5229026811659488\n",
            "2022-12-07 17:41:56.363446 Epoch 282, Training loss 0.5231630071959532\n",
            "2022-12-07 17:42:08.300747 Epoch 283, Training loss 0.5219115766570391\n",
            "2022-12-07 17:42:20.409058 Epoch 284, Training loss 0.5223311100469525\n",
            "2022-12-07 17:42:32.496513 Epoch 285, Training loss 0.522190560198501\n",
            "2022-12-07 17:42:44.624741 Epoch 286, Training loss 0.5207494247676162\n",
            "2022-12-07 17:42:56.822374 Epoch 287, Training loss 0.5192046539519753\n",
            "2022-12-07 17:43:08.777662 Epoch 288, Training loss 0.5199170207695278\n",
            "2022-12-07 17:43:20.844470 Epoch 289, Training loss 0.5189453660679595\n",
            "2022-12-07 17:43:32.904349 Epoch 290, Training loss 0.52024076484582\n",
            "2022-12-07 17:43:45.075025 Epoch 291, Training loss 0.5170354994247331\n",
            "2022-12-07 17:43:57.320021 Epoch 292, Training loss 0.519191895547273\n",
            "2022-12-07 17:44:09.532574 Epoch 293, Training loss 0.5166793017626723\n",
            "2022-12-07 17:44:21.865759 Epoch 294, Training loss 0.5183320268798057\n",
            "2022-12-07 17:44:34.063721 Epoch 295, Training loss 0.5178363947269252\n",
            "2022-12-07 17:44:46.374549 Epoch 296, Training loss 0.5178129542285524\n",
            "2022-12-07 17:44:58.495949 Epoch 297, Training loss 0.5166334428102769\n",
            "2022-12-07 17:45:10.649556 Epoch 298, Training loss 0.5177923685220807\n",
            "2022-12-07 17:45:22.907272 Epoch 299, Training loss 0.514983625058323\n",
            "2022-12-07 17:45:35.069314 Epoch 300, Training loss 0.5130001051765879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                          shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                        shuffle=False)\n",
        "validate(model, train_loader, val_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i165rtzpeOlP",
        "outputId": "b5415e72-5bae-4675-fa59-eed6c6670ed2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.81\n",
            "Accuracy val: 0.62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part B"
      ],
      "metadata": {
        "id": "pSmsi3sCseXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.act1 = nn.Tanh()\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "        self.act2 = nn.Tanh()\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.conv3 = nn.Conv2d(8, 3, kernel_size=3, padding=1)\n",
        "        self.act3 = nn.Tanh()\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
        "        self.act3 = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.pool1(self.act1(self.conv1(x)))\n",
        "        out = self.pool2(self.act2(self.conv2(out)))\n",
        "        out = out.view(-1, 8 * 8 * 8) # <1>\n",
        "        out = self.act3(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "XRs6eHQPsf7T"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                        shuffle=True)\n",
        "model = Net().to(device=device)  #  <2>\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)  #  <3>\n",
        "loss_fn = nn.CrossEntropyLoss()  #  <4>\n",
        "\n",
        "training_loop(  # <5>\n",
        "    n_epochs = 300,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyNdwt4WtZpR",
        "outputId": "e58311db-4fbb-4ca8-85d8-6841789e7a58"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-12-07 17:50:31.445862 Epoch 1, Training loss 2.0514538217993343\n",
            "2022-12-07 17:50:43.460893 Epoch 2, Training loss 1.7980358996964476\n",
            "2022-12-07 17:50:55.384909 Epoch 3, Training loss 1.6203605169835298\n",
            "2022-12-07 17:51:07.440745 Epoch 4, Training loss 1.5030134390382206\n",
            "2022-12-07 17:51:19.458525 Epoch 5, Training loss 1.415657628070363\n",
            "2022-12-07 17:51:31.457748 Epoch 6, Training loss 1.3448547268919933\n",
            "2022-12-07 17:51:43.506264 Epoch 7, Training loss 1.2895419903271033\n",
            "2022-12-07 17:51:55.412668 Epoch 8, Training loss 1.2466653861353159\n",
            "2022-12-07 17:52:07.502946 Epoch 9, Training loss 1.211783008349826\n",
            "2022-12-07 17:52:19.485119 Epoch 10, Training loss 1.1828700975536386\n",
            "2022-12-07 17:52:31.547303 Epoch 11, Training loss 1.1574098905334083\n",
            "2022-12-07 17:52:43.601510 Epoch 12, Training loss 1.1359829952192428\n",
            "2022-12-07 17:52:55.486710 Epoch 13, Training loss 1.117091161744369\n",
            "2022-12-07 17:53:07.547972 Epoch 14, Training loss 1.0998158029583105\n",
            "2022-12-07 17:53:19.431162 Epoch 15, Training loss 1.0824898364751234\n",
            "2022-12-07 17:53:31.360542 Epoch 16, Training loss 1.0680008709735578\n",
            "2022-12-07 17:53:43.331602 Epoch 17, Training loss 1.054954520271867\n",
            "2022-12-07 17:53:55.224169 Epoch 18, Training loss 1.0398958086815027\n",
            "2022-12-07 17:54:07.283272 Epoch 19, Training loss 1.028089528574663\n",
            "2022-12-07 17:54:19.235224 Epoch 20, Training loss 1.0172656323293896\n",
            "2022-12-07 17:54:31.309592 Epoch 21, Training loss 1.004488808877023\n",
            "2022-12-07 17:54:43.358666 Epoch 22, Training loss 0.9934564570484259\n",
            "2022-12-07 17:54:55.395429 Epoch 23, Training loss 0.9835753364636161\n",
            "2022-12-07 17:55:07.390382 Epoch 24, Training loss 0.9740428674556411\n",
            "2022-12-07 17:55:19.294391 Epoch 25, Training loss 0.9628763932858586\n",
            "2022-12-07 17:55:31.390504 Epoch 26, Training loss 0.9554507604340459\n",
            "2022-12-07 17:55:43.403456 Epoch 27, Training loss 0.9458762216750923\n",
            "2022-12-07 17:55:55.562369 Epoch 28, Training loss 0.9394980625575765\n",
            "2022-12-07 17:56:07.582096 Epoch 29, Training loss 0.9306969604528773\n",
            "2022-12-07 17:56:19.439188 Epoch 30, Training loss 0.9244123422123892\n",
            "2022-12-07 17:56:31.542653 Epoch 31, Training loss 0.9155651486819357\n",
            "2022-12-07 17:56:43.499266 Epoch 32, Training loss 0.9104130351177567\n",
            "2022-12-07 17:56:55.487617 Epoch 33, Training loss 0.9024396282632637\n",
            "2022-12-07 17:57:07.439164 Epoch 34, Training loss 0.8957062148681992\n",
            "2022-12-07 17:57:19.423840 Epoch 35, Training loss 0.8910693947006675\n",
            "2022-12-07 17:57:31.605627 Epoch 36, Training loss 0.8839670302313002\n",
            "2022-12-07 17:57:43.516077 Epoch 37, Training loss 0.8783793835078969\n",
            "2022-12-07 17:57:55.523654 Epoch 38, Training loss 0.8720759100392651\n",
            "2022-12-07 17:58:07.482375 Epoch 39, Training loss 0.866003533382245\n",
            "2022-12-07 17:58:19.377797 Epoch 40, Training loss 0.8625780919476238\n",
            "2022-12-07 17:58:31.356483 Epoch 41, Training loss 0.8564506830919124\n",
            "2022-12-07 17:58:43.293162 Epoch 42, Training loss 0.8517369798687108\n",
            "2022-12-07 17:58:55.292429 Epoch 43, Training loss 0.8461998713290905\n",
            "2022-12-07 17:59:07.197370 Epoch 44, Training loss 0.8427685375713632\n",
            "2022-12-07 17:59:19.203372 Epoch 45, Training loss 0.8366950264062418\n",
            "2022-12-07 17:59:31.177196 Epoch 46, Training loss 0.8337236441995787\n",
            "2022-12-07 17:59:43.097635 Epoch 47, Training loss 0.8296684664304909\n",
            "2022-12-07 17:59:55.353227 Epoch 48, Training loss 0.8253133083548387\n",
            "2022-12-07 18:00:07.349803 Epoch 49, Training loss 0.8218202116849173\n",
            "2022-12-07 18:00:19.215381 Epoch 50, Training loss 0.8163972963838626\n",
            "2022-12-07 18:00:31.236033 Epoch 51, Training loss 0.8129953141407589\n",
            "2022-12-07 18:00:43.242191 Epoch 52, Training loss 0.8100190449629903\n",
            "2022-12-07 18:00:55.372829 Epoch 53, Training loss 0.8063241209825287\n",
            "2022-12-07 18:01:07.388790 Epoch 54, Training loss 0.8019856882598394\n",
            "2022-12-07 18:01:19.514035 Epoch 55, Training loss 0.800300809428515\n",
            "2022-12-07 18:01:31.522761 Epoch 56, Training loss 0.795445047376101\n",
            "2022-12-07 18:01:43.454812 Epoch 57, Training loss 0.7917373052338506\n",
            "2022-12-07 18:01:55.541508 Epoch 58, Training loss 0.7894272673541628\n",
            "2022-12-07 18:02:07.380188 Epoch 59, Training loss 0.78701338179581\n",
            "2022-12-07 18:02:19.715356 Epoch 60, Training loss 0.7831908599723636\n",
            "2022-12-07 18:02:31.662372 Epoch 61, Training loss 0.7805545282409624\n",
            "2022-12-07 18:02:43.510314 Epoch 62, Training loss 0.7766467753578635\n",
            "2022-12-07 18:02:55.459013 Epoch 63, Training loss 0.773204525260974\n",
            "2022-12-07 18:03:07.484970 Epoch 64, Training loss 0.77106571319463\n",
            "2022-12-07 18:03:19.677877 Epoch 65, Training loss 0.7690922036347791\n",
            "2022-12-07 18:03:31.553680 Epoch 66, Training loss 0.7650238878434271\n",
            "2022-12-07 18:03:43.665405 Epoch 67, Training loss 0.7630917235561039\n",
            "2022-12-07 18:03:55.617771 Epoch 68, Training loss 0.7595224100381822\n",
            "2022-12-07 18:04:07.510083 Epoch 69, Training loss 0.7574600812876621\n",
            "2022-12-07 18:04:19.530932 Epoch 70, Training loss 0.7549153681072738\n",
            "2022-12-07 18:04:31.529054 Epoch 71, Training loss 0.7529624905199042\n",
            "2022-12-07 18:04:43.481148 Epoch 72, Training loss 0.7496490818841378\n",
            "2022-12-07 18:04:55.517221 Epoch 73, Training loss 0.7458928369957468\n",
            "2022-12-07 18:05:07.436817 Epoch 74, Training loss 0.7442874800594871\n",
            "2022-12-07 18:05:19.448395 Epoch 75, Training loss 0.7419736644496089\n",
            "2022-12-07 18:05:31.367185 Epoch 76, Training loss 0.7387372203495192\n",
            "2022-12-07 18:05:43.439648 Epoch 77, Training loss 0.7376896847620644\n",
            "2022-12-07 18:05:55.303556 Epoch 78, Training loss 0.7343010374957033\n",
            "2022-12-07 18:06:07.176712 Epoch 79, Training loss 0.7319618429597992\n",
            "2022-12-07 18:06:19.247162 Epoch 80, Training loss 0.7308298381012114\n",
            "2022-12-07 18:06:31.120312 Epoch 81, Training loss 0.7276011253790478\n",
            "2022-12-07 18:06:43.044108 Epoch 82, Training loss 0.7253167458888515\n",
            "2022-12-07 18:06:55.067803 Epoch 83, Training loss 0.7248107958251558\n",
            "2022-12-07 18:07:07.001818 Epoch 84, Training loss 0.7202507284138818\n",
            "2022-12-07 18:07:19.075007 Epoch 85, Training loss 0.7200828669092539\n",
            "2022-12-07 18:07:30.969880 Epoch 86, Training loss 0.7164715154625266\n",
            "2022-12-07 18:07:43.059605 Epoch 87, Training loss 0.7140613286315328\n",
            "2022-12-07 18:07:55.132164 Epoch 88, Training loss 0.7118837402757171\n",
            "2022-12-07 18:08:06.998639 Epoch 89, Training loss 0.7088958775753256\n",
            "2022-12-07 18:08:19.232400 Epoch 90, Training loss 0.7086661993466374\n",
            "2022-12-07 18:08:31.187037 Epoch 91, Training loss 0.7066616582519868\n",
            "2022-12-07 18:08:43.189866 Epoch 92, Training loss 0.7033224886716785\n",
            "2022-12-07 18:08:55.158144 Epoch 93, Training loss 0.7028582748168569\n",
            "2022-12-07 18:09:07.233574 Epoch 94, Training loss 0.7025060671598405\n",
            "2022-12-07 18:09:19.253326 Epoch 95, Training loss 0.699939361732939\n",
            "2022-12-07 18:09:31.297111 Epoch 96, Training loss 0.6964038622272594\n",
            "2022-12-07 18:09:43.438092 Epoch 97, Training loss 0.6949512190221215\n",
            "2022-12-07 18:09:55.369381 Epoch 98, Training loss 0.6926458301141744\n",
            "2022-12-07 18:10:07.320864 Epoch 99, Training loss 0.6902623302338983\n",
            "2022-12-07 18:10:19.452863 Epoch 100, Training loss 0.6903018221221007\n",
            "2022-12-07 18:10:31.344490 Epoch 101, Training loss 0.688461331150416\n",
            "2022-12-07 18:10:43.468365 Epoch 102, Training loss 0.6863025043855238\n",
            "2022-12-07 18:10:55.279194 Epoch 103, Training loss 0.6840991456719006\n",
            "2022-12-07 18:11:07.212079 Epoch 104, Training loss 0.6826904959919508\n",
            "2022-12-07 18:11:19.262661 Epoch 105, Training loss 0.6808870113109384\n",
            "2022-12-07 18:11:31.277776 Epoch 106, Training loss 0.6799106446221052\n",
            "2022-12-07 18:11:43.413540 Epoch 107, Training loss 0.6785609522820129\n",
            "2022-12-07 18:11:55.377265 Epoch 108, Training loss 0.6762179323779348\n",
            "2022-12-07 18:12:07.620367 Epoch 109, Training loss 0.6736867446881121\n",
            "2022-12-07 18:12:19.523335 Epoch 110, Training loss 0.6737167281491677\n",
            "2022-12-07 18:12:31.444123 Epoch 111, Training loss 0.6714562608305451\n",
            "2022-12-07 18:12:43.532203 Epoch 112, Training loss 0.669565612474061\n",
            "2022-12-07 18:12:55.482160 Epoch 113, Training loss 0.6681737551070235\n",
            "2022-12-07 18:13:07.498222 Epoch 114, Training loss 0.6669399765370142\n",
            "2022-12-07 18:13:19.411887 Epoch 115, Training loss 0.6649307676822024\n",
            "2022-12-07 18:13:31.597276 Epoch 116, Training loss 0.6622689139202732\n",
            "2022-12-07 18:13:43.731861 Epoch 117, Training loss 0.6628685275383313\n",
            "2022-12-07 18:13:55.628153 Epoch 118, Training loss 0.6606324574221736\n",
            "2022-12-07 18:14:07.707447 Epoch 119, Training loss 0.6597210709243784\n",
            "2022-12-07 18:14:19.599604 Epoch 120, Training loss 0.6581012386723858\n",
            "2022-12-07 18:14:31.651458 Epoch 121, Training loss 0.6570203865656767\n",
            "2022-12-07 18:14:43.786648 Epoch 122, Training loss 0.6546850982300766\n",
            "2022-12-07 18:14:55.765140 Epoch 123, Training loss 0.653683705898502\n",
            "2022-12-07 18:15:07.795107 Epoch 124, Training loss 0.6520205534175229\n",
            "2022-12-07 18:15:19.617501 Epoch 125, Training loss 0.6503099139465396\n",
            "2022-12-07 18:15:31.563478 Epoch 126, Training loss 0.6510897584812111\n",
            "2022-12-07 18:15:43.506261 Epoch 127, Training loss 0.6496394982042215\n",
            "2022-12-07 18:15:55.483233 Epoch 128, Training loss 0.6478644907474518\n",
            "2022-12-07 18:16:07.761047 Epoch 129, Training loss 0.6454337239265442\n",
            "2022-12-07 18:16:19.655121 Epoch 130, Training loss 0.646267723396916\n",
            "2022-12-07 18:16:31.642810 Epoch 131, Training loss 0.6437811389984682\n",
            "2022-12-07 18:16:43.712174 Epoch 132, Training loss 0.6423917126167765\n",
            "2022-12-07 18:16:55.749476 Epoch 133, Training loss 0.64321291862089\n",
            "2022-12-07 18:17:07.729170 Epoch 134, Training loss 0.6398316447997032\n",
            "2022-12-07 18:17:19.588868 Epoch 135, Training loss 0.6395189205703833\n",
            "2022-12-07 18:17:31.652031 Epoch 136, Training loss 0.6372017999134405\n",
            "2022-12-07 18:17:43.645884 Epoch 137, Training loss 0.6359703706963288\n",
            "2022-12-07 18:17:55.651422 Epoch 138, Training loss 0.6352541773291804\n",
            "2022-12-07 18:18:07.582397 Epoch 139, Training loss 0.633045175724932\n",
            "2022-12-07 18:18:19.500317 Epoch 140, Training loss 0.6329978976941779\n",
            "2022-12-07 18:18:31.618271 Epoch 141, Training loss 0.6315202727494642\n",
            "2022-12-07 18:18:43.503491 Epoch 142, Training loss 0.6308444540976258\n",
            "2022-12-07 18:18:55.532190 Epoch 143, Training loss 0.6305532994706308\n",
            "2022-12-07 18:19:07.468785 Epoch 144, Training loss 0.6291186976463289\n",
            "2022-12-07 18:19:19.443480 Epoch 145, Training loss 0.6283027900530554\n",
            "2022-12-07 18:19:31.622065 Epoch 146, Training loss 0.6267711538869096\n",
            "2022-12-07 18:19:43.610490 Epoch 147, Training loss 0.6248231277136547\n",
            "2022-12-07 18:19:55.616105 Epoch 148, Training loss 0.6255429428251807\n",
            "2022-12-07 18:20:07.705338 Epoch 149, Training loss 0.6235445685818067\n",
            "2022-12-07 18:20:19.596233 Epoch 150, Training loss 0.6218791834990997\n",
            "2022-12-07 18:20:31.618510 Epoch 151, Training loss 0.6196881549818741\n",
            "2022-12-07 18:20:43.641267 Epoch 152, Training loss 0.6202577124623692\n",
            "2022-12-07 18:20:55.729435 Epoch 153, Training loss 0.6182140237687493\n",
            "2022-12-07 18:21:07.762652 Epoch 154, Training loss 0.6164076627825227\n",
            "2022-12-07 18:21:19.756442 Epoch 155, Training loss 0.6172743070384731\n",
            "2022-12-07 18:21:31.665767 Epoch 156, Training loss 0.6145929772683116\n",
            "2022-12-07 18:21:43.651519 Epoch 157, Training loss 0.6148870811819116\n",
            "2022-12-07 18:21:55.833391 Epoch 158, Training loss 0.6124477342266561\n",
            "2022-12-07 18:22:07.747042 Epoch 159, Training loss 0.6137699134971785\n",
            "2022-12-07 18:22:19.709431 Epoch 160, Training loss 0.6109788405239734\n",
            "2022-12-07 18:22:31.651388 Epoch 161, Training loss 0.610374237532201\n",
            "2022-12-07 18:22:43.632497 Epoch 162, Training loss 0.6104020574284942\n",
            "2022-12-07 18:22:55.742647 Epoch 163, Training loss 0.6087424080923695\n",
            "2022-12-07 18:23:07.713065 Epoch 164, Training loss 0.6076987748560698\n",
            "2022-12-07 18:23:19.740550 Epoch 165, Training loss 0.60656199373705\n",
            "2022-12-07 18:23:31.675071 Epoch 166, Training loss 0.6068250860094719\n",
            "2022-12-07 18:23:43.650864 Epoch 167, Training loss 0.605408444352772\n",
            "2022-12-07 18:23:55.660720 Epoch 168, Training loss 0.6051873740027932\n",
            "2022-12-07 18:24:07.550914 Epoch 169, Training loss 0.6022870187335612\n",
            "2022-12-07 18:24:19.630043 Epoch 170, Training loss 0.6025202412663213\n",
            "2022-12-07 18:24:31.550609 Epoch 171, Training loss 0.6024602241909413\n",
            "2022-12-07 18:24:43.445034 Epoch 172, Training loss 0.6016296319034703\n",
            "2022-12-07 18:24:55.463562 Epoch 173, Training loss 0.6001705095133818\n",
            "2022-12-07 18:25:07.505629 Epoch 174, Training loss 0.5975664578130483\n",
            "2022-12-07 18:25:19.541574 Epoch 175, Training loss 0.5973637461128747\n",
            "2022-12-07 18:25:31.442489 Epoch 176, Training loss 0.5968133245435212\n",
            "2022-12-07 18:25:43.525661 Epoch 177, Training loss 0.5969929364331238\n",
            "2022-12-07 18:25:55.583810 Epoch 178, Training loss 0.5961200264103882\n",
            "2022-12-07 18:26:07.596208 Epoch 179, Training loss 0.5945504725055621\n",
            "2022-12-07 18:26:19.686372 Epoch 180, Training loss 0.5941807181786394\n",
            "2022-12-07 18:26:31.634497 Epoch 181, Training loss 0.5931827465591528\n",
            "2022-12-07 18:26:43.806564 Epoch 182, Training loss 0.5934821174425238\n",
            "2022-12-07 18:26:55.825709 Epoch 183, Training loss 0.5917495755512087\n",
            "2022-12-07 18:27:07.849750 Epoch 184, Training loss 0.5899507499030788\n",
            "2022-12-07 18:27:19.902128 Epoch 185, Training loss 0.5891239331735064\n",
            "2022-12-07 18:27:31.844889 Epoch 186, Training loss 0.5891703784542011\n",
            "2022-12-07 18:27:43.906209 Epoch 187, Training loss 0.5883039199482755\n",
            "2022-12-07 18:27:55.793513 Epoch 188, Training loss 0.588482534241341\n",
            "2022-12-07 18:28:07.712775 Epoch 189, Training loss 0.5876532037316076\n",
            "2022-12-07 18:28:19.632434 Epoch 190, Training loss 0.5877280594671473\n",
            "2022-12-07 18:28:31.550674 Epoch 191, Training loss 0.5862861969663055\n",
            "2022-12-07 18:28:43.575710 Epoch 192, Training loss 0.5834123586754665\n",
            "2022-12-07 18:28:55.581114 Epoch 193, Training loss 0.5834427676770998\n",
            "2022-12-07 18:29:07.736511 Epoch 194, Training loss 0.5817762764594744\n",
            "2022-12-07 18:29:19.728653 Epoch 195, Training loss 0.58264490567586\n",
            "2022-12-07 18:29:31.699929 Epoch 196, Training loss 0.5822007928586677\n",
            "2022-12-07 18:29:43.767026 Epoch 197, Training loss 0.5807876398267648\n",
            "2022-12-07 18:29:55.859548 Epoch 198, Training loss 0.5809714241939432\n",
            "2022-12-07 18:30:07.804267 Epoch 199, Training loss 0.5801795279354696\n",
            "2022-12-07 18:30:19.928757 Epoch 200, Training loss 0.5782534424834849\n",
            "2022-12-07 18:30:31.942242 Epoch 201, Training loss 0.5780796195235094\n",
            "2022-12-07 18:30:43.891679 Epoch 202, Training loss 0.5774608054353149\n",
            "2022-12-07 18:30:55.964062 Epoch 203, Training loss 0.5770135433091532\n",
            "2022-12-07 18:31:07.970179 Epoch 204, Training loss 0.576354822577418\n",
            "2022-12-07 18:31:20.050714 Epoch 205, Training loss 0.5757274492774778\n",
            "2022-12-07 18:31:31.967888 Epoch 206, Training loss 0.5722289062903055\n",
            "2022-12-07 18:31:43.964346 Epoch 207, Training loss 0.5732333778267931\n",
            "2022-12-07 18:31:56.009245 Epoch 208, Training loss 0.5735794300465937\n",
            "2022-12-07 18:32:07.942569 Epoch 209, Training loss 0.5719907709285427\n",
            "2022-12-07 18:32:20.067290 Epoch 210, Training loss 0.5722728942894875\n",
            "2022-12-07 18:32:32.136423 Epoch 211, Training loss 0.5707099367971615\n",
            "2022-12-07 18:32:44.084192 Epoch 212, Training loss 0.5694084100025084\n",
            "2022-12-07 18:32:56.135453 Epoch 213, Training loss 0.5703359841919311\n",
            "2022-12-07 18:33:08.223568 Epoch 214, Training loss 0.5690028928887204\n",
            "2022-12-07 18:33:20.297500 Epoch 215, Training loss 0.5676368815667184\n",
            "2022-12-07 18:33:32.267279 Epoch 216, Training loss 0.5679816008185792\n",
            "2022-12-07 18:33:44.221340 Epoch 217, Training loss 0.566066125736517\n",
            "2022-12-07 18:33:56.370771 Epoch 218, Training loss 0.5672193268680816\n",
            "2022-12-07 18:34:08.365404 Epoch 219, Training loss 0.5655735071052981\n",
            "2022-12-07 18:34:20.368344 Epoch 220, Training loss 0.5654027429230682\n",
            "2022-12-07 18:34:32.352994 Epoch 221, Training loss 0.5648078046491384\n",
            "2022-12-07 18:34:44.238378 Epoch 222, Training loss 0.5627643231426358\n",
            "2022-12-07 18:34:56.345926 Epoch 223, Training loss 0.5634213705806781\n",
            "2022-12-07 18:35:08.298804 Epoch 224, Training loss 0.562750809561566\n",
            "2022-12-07 18:35:20.263399 Epoch 225, Training loss 0.56303590974387\n",
            "2022-12-07 18:35:32.246669 Epoch 226, Training loss 0.5617795117447139\n",
            "2022-12-07 18:35:44.179078 Epoch 227, Training loss 0.5618472164854065\n",
            "2022-12-07 18:35:56.276196 Epoch 228, Training loss 0.5589766022761155\n",
            "2022-12-07 18:36:08.206219 Epoch 229, Training loss 0.5598051392894876\n",
            "2022-12-07 18:36:20.248445 Epoch 230, Training loss 0.5588837063221066\n",
            "2022-12-07 18:36:32.249377 Epoch 231, Training loss 0.5581997224055898\n",
            "2022-12-07 18:36:44.198839 Epoch 232, Training loss 0.5564999455381232\n",
            "2022-12-07 18:36:56.321543 Epoch 233, Training loss 0.555964310646362\n",
            "2022-12-07 18:37:08.196249 Epoch 234, Training loss 0.5573306657240519\n",
            "2022-12-07 18:37:20.338987 Epoch 235, Training loss 0.5554732486719975\n",
            "2022-12-07 18:37:32.359118 Epoch 236, Training loss 0.5544538581005448\n",
            "2022-12-07 18:37:44.245343 Epoch 237, Training loss 0.5538285406273039\n",
            "2022-12-07 18:37:56.319622 Epoch 238, Training loss 0.5558939296799852\n",
            "2022-12-07 18:38:08.350142 Epoch 239, Training loss 0.5533430692561142\n",
            "2022-12-07 18:38:20.428520 Epoch 240, Training loss 0.5539418720756956\n",
            "2022-12-07 18:38:32.420042 Epoch 241, Training loss 0.5527384609669981\n",
            "2022-12-07 18:38:44.412713 Epoch 242, Training loss 0.5536249282643618\n",
            "2022-12-07 18:38:56.444016 Epoch 243, Training loss 0.5519385223879534\n",
            "2022-12-07 18:39:08.391089 Epoch 244, Training loss 0.5497442626434824\n",
            "2022-12-07 18:39:20.420807 Epoch 245, Training loss 0.5510042977645574\n",
            "2022-12-07 18:39:32.334575 Epoch 246, Training loss 0.5500210109726548\n",
            "2022-12-07 18:39:44.330380 Epoch 247, Training loss 0.549326950136353\n",
            "2022-12-07 18:39:56.347070 Epoch 248, Training loss 0.5485457224614175\n",
            "2022-12-07 18:40:08.227049 Epoch 249, Training loss 0.5474472342016142\n",
            "2022-12-07 18:40:20.210880 Epoch 250, Training loss 0.5466533435122741\n",
            "2022-12-07 18:40:32.267032 Epoch 251, Training loss 0.5485267446702703\n",
            "2022-12-07 18:40:44.285551 Epoch 252, Training loss 0.5462669822794702\n",
            "2022-12-07 18:40:56.374099 Epoch 253, Training loss 0.5470192110561349\n",
            "2022-12-07 18:41:08.377707 Epoch 254, Training loss 0.5453650691663213\n",
            "2022-12-07 18:41:20.479655 Epoch 255, Training loss 0.5441765326749334\n",
            "2022-12-07 18:41:32.477707 Epoch 256, Training loss 0.5442540221430762\n",
            "2022-12-07 18:41:44.521944 Epoch 257, Training loss 0.543562122127589\n",
            "2022-12-07 18:41:56.574197 Epoch 258, Training loss 0.5431882478773137\n",
            "2022-12-07 18:42:08.535041 Epoch 259, Training loss 0.5425483005888322\n",
            "2022-12-07 18:42:20.681994 Epoch 260, Training loss 0.5432494187637058\n",
            "2022-12-07 18:42:32.657789 Epoch 261, Training loss 0.543704701392242\n",
            "2022-12-07 18:42:44.564772 Epoch 262, Training loss 0.5413089846558583\n",
            "2022-12-07 18:42:56.523966 Epoch 263, Training loss 0.5418288180666506\n",
            "2022-12-07 18:43:08.443333 Epoch 264, Training loss 0.5390562824428539\n",
            "2022-12-07 18:43:20.473812 Epoch 265, Training loss 0.5405257465055836\n",
            "2022-12-07 18:43:32.467281 Epoch 266, Training loss 0.538415936557838\n",
            "2022-12-07 18:43:44.464017 Epoch 267, Training loss 0.5371842090888401\n",
            "2022-12-07 18:43:56.413603 Epoch 268, Training loss 0.5382065402005639\n",
            "2022-12-07 18:44:08.266536 Epoch 269, Training loss 0.5377873531768999\n",
            "2022-12-07 18:44:20.308545 Epoch 270, Training loss 0.5368027664206522\n",
            "2022-12-07 18:44:32.251594 Epoch 271, Training loss 0.5366623167834623\n",
            "2022-12-07 18:44:44.301071 Epoch 272, Training loss 0.5373528662240109\n",
            "2022-12-07 18:44:56.323241 Epoch 273, Training loss 0.5364923504993434\n",
            "2022-12-07 18:45:08.245184 Epoch 274, Training loss 0.5348473013476338\n",
            "2022-12-07 18:45:20.239259 Epoch 275, Training loss 0.5356492638359289\n",
            "2022-12-07 18:45:32.249382 Epoch 276, Training loss 0.5337159313890331\n",
            "2022-12-07 18:45:44.302351 Epoch 277, Training loss 0.5353815973071796\n",
            "2022-12-07 18:45:56.311745 Epoch 278, Training loss 0.5326384732409206\n",
            "2022-12-07 18:46:08.162788 Epoch 279, Training loss 0.5328154433756838\n",
            "2022-12-07 18:46:20.186808 Epoch 280, Training loss 0.5318031729487203\n",
            "2022-12-07 18:46:32.110321 Epoch 281, Training loss 0.5317611457289332\n",
            "2022-12-07 18:46:44.212361 Epoch 282, Training loss 0.5308509559925559\n",
            "2022-12-07 18:46:56.265770 Epoch 283, Training loss 0.5313064474660112\n",
            "2022-12-07 18:47:08.333669 Epoch 284, Training loss 0.5320135713805019\n",
            "2022-12-07 18:47:20.522450 Epoch 285, Training loss 0.5300978063546178\n",
            "2022-12-07 18:47:32.515995 Epoch 286, Training loss 0.5288872485575469\n",
            "2022-12-07 18:47:44.634121 Epoch 287, Training loss 0.5302504590329002\n",
            "2022-12-07 18:47:56.821850 Epoch 288, Training loss 0.5298042656363123\n",
            "2022-12-07 18:48:08.770779 Epoch 289, Training loss 0.5289190126120892\n",
            "2022-12-07 18:48:20.833911 Epoch 290, Training loss 0.5271322804376902\n",
            "2022-12-07 18:48:32.815118 Epoch 291, Training loss 0.5279171181380596\n",
            "2022-12-07 18:48:44.901764 Epoch 292, Training loss 0.5284628063973869\n",
            "2022-12-07 18:48:56.821996 Epoch 293, Training loss 0.5258022517042087\n",
            "2022-12-07 18:49:08.728758 Epoch 294, Training loss 0.5239533612795193\n",
            "2022-12-07 18:49:20.701079 Epoch 295, Training loss 0.5247972858378954\n",
            "2022-12-07 18:49:32.673830 Epoch 296, Training loss 0.5244956733968557\n",
            "2022-12-07 18:49:44.825320 Epoch 297, Training loss 0.5254779574663743\n",
            "2022-12-07 18:49:56.814365 Epoch 298, Training loss 0.5241798993266756\n",
            "2022-12-07 18:50:08.778191 Epoch 299, Training loss 0.5242582382943929\n",
            "2022-12-07 18:50:20.847887 Epoch 300, Training loss 0.5242612814659353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                          shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                        shuffle=False)\n",
        "validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvKW0w6jEaaF",
        "outputId": "a4a009f9-fd17-4204-b71b-25f1b3d7588a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.76\n",
            "Accuracy val: 0.60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 2"
      ],
      "metadata": {
        "id": "0d3mnifREiVM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part A"
      ],
      "metadata": {
        "id": "oaJA4IbiEk9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NetRes(nn.Module):\n",
        "    def __init__(self, n_chans1=32):\n",
        "        super().__init__()\n",
        "        self.n_chans1 = n_chans1\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
        "                               padding=1)\n",
        "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2,\n",
        "                               kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
        "        out1 = out\n",
        "        out = F.max_pool2d(torch.relu(self.conv3(out)) + out1, 2)\n",
        "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
        "        out = torch.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "gRZEGCLwEj0X"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                           shuffle=True)\n",
        "model = NetRes(n_chans1=32).to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 300,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SK8no7SEE_hv",
        "outputId": "9fda02ac-e4b4-4f80-c716-253a5ba1f5c2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-12-07 19:33:56.468503 Epoch 1, Training loss 2.0726132075804884\n",
            "2022-12-07 19:34:08.964743 Epoch 2, Training loss 1.7108220932123912\n",
            "2022-12-07 19:34:21.501335 Epoch 3, Training loss 1.522366407277334\n",
            "2022-12-07 19:34:33.854186 Epoch 4, Training loss 1.4175503389609745\n",
            "2022-12-07 19:34:46.402777 Epoch 5, Training loss 1.3382294338834866\n",
            "2022-12-07 19:34:58.755141 Epoch 6, Training loss 1.2684047984345186\n",
            "2022-12-07 19:35:11.276311 Epoch 7, Training loss 1.211839218959784\n",
            "2022-12-07 19:35:23.592098 Epoch 8, Training loss 1.1649688265055342\n",
            "2022-12-07 19:35:35.995739 Epoch 9, Training loss 1.121424007674922\n",
            "2022-12-07 19:35:48.358969 Epoch 10, Training loss 1.0826403432337524\n",
            "2022-12-07 19:36:00.640971 Epoch 11, Training loss 1.0535673619536183\n",
            "2022-12-07 19:36:13.113313 Epoch 12, Training loss 1.0236433068352282\n",
            "2022-12-07 19:36:25.554658 Epoch 13, Training loss 0.9989350533393948\n",
            "2022-12-07 19:36:38.194206 Epoch 14, Training loss 0.9772031695184196\n",
            "2022-12-07 19:36:50.573838 Epoch 15, Training loss 0.9590656767263437\n",
            "2022-12-07 19:37:02.909682 Epoch 16, Training loss 0.9421676153417133\n",
            "2022-12-07 19:37:15.433749 Epoch 17, Training loss 0.9290176578952224\n",
            "2022-12-07 19:37:27.747360 Epoch 18, Training loss 0.9142119246523094\n",
            "2022-12-07 19:37:40.232012 Epoch 19, Training loss 0.8956429942718247\n",
            "2022-12-07 19:37:52.610622 Epoch 20, Training loss 0.885091623534327\n",
            "2022-12-07 19:38:05.086655 Epoch 21, Training loss 0.8702777123359768\n",
            "2022-12-07 19:38:17.565091 Epoch 22, Training loss 0.8612266081525847\n",
            "2022-12-07 19:38:29.909219 Epoch 23, Training loss 0.8488786176342489\n",
            "2022-12-07 19:38:42.443758 Epoch 24, Training loss 0.8396286840938851\n",
            "2022-12-07 19:38:54.861068 Epoch 25, Training loss 0.8313853799763238\n",
            "2022-12-07 19:39:07.522327 Epoch 26, Training loss 0.8206882098751604\n",
            "2022-12-07 19:39:19.990627 Epoch 27, Training loss 0.809711980316645\n",
            "2022-12-07 19:39:32.383283 Epoch 28, Training loss 0.8001126442342767\n",
            "2022-12-07 19:39:44.956085 Epoch 29, Training loss 0.7937433282508874\n",
            "2022-12-07 19:39:57.292552 Epoch 30, Training loss 0.7828525490772998\n",
            "2022-12-07 19:40:09.737752 Epoch 31, Training loss 0.7760180973488352\n",
            "2022-12-07 19:40:22.163282 Epoch 32, Training loss 0.7703436769335471\n",
            "2022-12-07 19:40:34.632464 Epoch 33, Training loss 0.7637409002274809\n",
            "2022-12-07 19:40:47.070110 Epoch 34, Training loss 0.7551826062562216\n",
            "2022-12-07 19:40:59.492063 Epoch 35, Training loss 0.7492480087463204\n",
            "2022-12-07 19:41:11.963981 Epoch 36, Training loss 0.7413552171357757\n",
            "2022-12-07 19:41:24.334015 Epoch 37, Training loss 0.739564101676197\n",
            "2022-12-07 19:41:36.972632 Epoch 38, Training loss 0.7304000514166434\n",
            "2022-12-07 19:41:49.406995 Epoch 39, Training loss 0.724279369349065\n",
            "2022-12-07 19:42:01.920737 Epoch 40, Training loss 0.7192478944807101\n",
            "2022-12-07 19:42:14.321777 Epoch 41, Training loss 0.7144212868741101\n",
            "2022-12-07 19:42:26.607053 Epoch 42, Training loss 0.7094104383760096\n",
            "2022-12-07 19:42:39.225161 Epoch 43, Training loss 0.702229241542804\n",
            "2022-12-07 19:42:51.593010 Epoch 44, Training loss 0.6986289719105376\n",
            "2022-12-07 19:43:04.114502 Epoch 45, Training loss 0.6939474399132497\n",
            "2022-12-07 19:43:16.518008 Epoch 46, Training loss 0.6881014704704285\n",
            "2022-12-07 19:43:28.943575 Epoch 47, Training loss 0.6848231409593006\n",
            "2022-12-07 19:43:41.410461 Epoch 48, Training loss 0.6798452694931298\n",
            "2022-12-07 19:43:53.775606 Epoch 49, Training loss 0.6774160198467162\n",
            "2022-12-07 19:44:06.342886 Epoch 50, Training loss 0.6728008742756246\n",
            "2022-12-07 19:44:18.825368 Epoch 51, Training loss 0.6676011730337996\n",
            "2022-12-07 19:44:31.365214 Epoch 52, Training loss 0.662587924076773\n",
            "2022-12-07 19:44:43.795748 Epoch 53, Training loss 0.6602491357213701\n",
            "2022-12-07 19:44:56.226376 Epoch 54, Training loss 0.6568523954476237\n",
            "2022-12-07 19:45:08.719440 Epoch 55, Training loss 0.6521520433980791\n",
            "2022-12-07 19:45:21.135029 Epoch 56, Training loss 0.6505506705597538\n",
            "2022-12-07 19:45:33.671016 Epoch 57, Training loss 0.6456395158987216\n",
            "2022-12-07 19:45:46.062483 Epoch 58, Training loss 0.6407768657369077\n",
            "2022-12-07 19:45:58.538753 Epoch 59, Training loss 0.6385607584891722\n",
            "2022-12-07 19:46:10.979961 Epoch 60, Training loss 0.6359213440085921\n",
            "2022-12-07 19:46:23.452111 Epoch 61, Training loss 0.6313490709456642\n",
            "2022-12-07 19:46:35.998010 Epoch 62, Training loss 0.6269578049935953\n",
            "2022-12-07 19:46:48.425026 Epoch 63, Training loss 0.6253072681939206\n",
            "2022-12-07 19:47:00.798245 Epoch 64, Training loss 0.6220562161539521\n",
            "2022-12-07 19:47:13.100282 Epoch 65, Training loss 0.6226260802706184\n",
            "2022-12-07 19:47:25.432956 Epoch 66, Training loss 0.6148270278254433\n",
            "2022-12-07 19:47:37.877047 Epoch 67, Training loss 0.6142049704290107\n",
            "2022-12-07 19:47:50.184927 Epoch 68, Training loss 0.610075723789537\n",
            "2022-12-07 19:48:02.667705 Epoch 69, Training loss 0.6048741414952461\n",
            "2022-12-07 19:48:15.081268 Epoch 70, Training loss 0.6056087003339587\n",
            "2022-12-07 19:48:27.675244 Epoch 71, Training loss 0.6035884632859998\n",
            "2022-12-07 19:48:40.211820 Epoch 72, Training loss 0.600290733873082\n",
            "2022-12-07 19:48:52.789812 Epoch 73, Training loss 0.5978299667844382\n",
            "2022-12-07 19:49:05.270397 Epoch 74, Training loss 0.5942250688743713\n",
            "2022-12-07 19:49:17.583519 Epoch 75, Training loss 0.5933650324259268\n",
            "2022-12-07 19:49:29.983063 Epoch 76, Training loss 0.5900268412916861\n",
            "2022-12-07 19:49:42.423352 Epoch 77, Training loss 0.5882841175245812\n",
            "2022-12-07 19:49:55.046339 Epoch 78, Training loss 0.5840783144163963\n",
            "2022-12-07 19:50:07.510077 Epoch 79, Training loss 0.5829172448810104\n",
            "2022-12-07 19:50:19.863648 Epoch 80, Training loss 0.579896947497602\n",
            "2022-12-07 19:50:32.325548 Epoch 81, Training loss 0.5780259772868412\n",
            "2022-12-07 19:50:44.671492 Epoch 82, Training loss 0.5750905449509316\n",
            "2022-12-07 19:50:57.227670 Epoch 83, Training loss 0.5740458884888597\n",
            "2022-12-07 19:51:09.649351 Epoch 84, Training loss 0.5732567036129019\n",
            "2022-12-07 19:51:22.211354 Epoch 85, Training loss 0.5726855259264827\n",
            "2022-12-07 19:51:34.896140 Epoch 86, Training loss 0.5678752243823713\n",
            "2022-12-07 19:51:47.340743 Epoch 87, Training loss 0.5658396180633389\n",
            "2022-12-07 19:52:00.026893 Epoch 88, Training loss 0.5637917204014481\n",
            "2022-12-07 19:52:12.401931 Epoch 89, Training loss 0.5622696852135232\n",
            "2022-12-07 19:52:24.887435 Epoch 90, Training loss 0.5595080780479914\n",
            "2022-12-07 19:52:37.362988 Epoch 91, Training loss 0.5564168543004624\n",
            "2022-12-07 19:52:49.848742 Epoch 92, Training loss 0.5546565752123933\n",
            "2022-12-07 19:53:02.306827 Epoch 93, Training loss 0.5552926005800362\n",
            "2022-12-07 19:53:14.612965 Epoch 94, Training loss 0.5505369907373663\n",
            "2022-12-07 19:53:27.121767 Epoch 95, Training loss 0.5488856616799179\n",
            "2022-12-07 19:53:39.690788 Epoch 96, Training loss 0.5481520022272759\n",
            "2022-12-07 19:53:52.580853 Epoch 97, Training loss 0.5497894015001215\n",
            "2022-12-07 19:54:05.102216 Epoch 98, Training loss 0.5469947951986357\n",
            "2022-12-07 19:54:17.521559 Epoch 99, Training loss 0.5426814486951475\n",
            "2022-12-07 19:54:30.022930 Epoch 100, Training loss 0.5417300992457154\n",
            "2022-12-07 19:54:42.377221 Epoch 101, Training loss 0.5413646388160603\n",
            "2022-12-07 19:54:54.902141 Epoch 102, Training loss 0.5390251042592861\n",
            "2022-12-07 19:55:07.306079 Epoch 103, Training loss 0.5369373243635572\n",
            "2022-12-07 19:55:19.815387 Epoch 104, Training loss 0.5350652325831716\n",
            "2022-12-07 19:55:32.288264 Epoch 105, Training loss 0.5315427637237418\n",
            "2022-12-07 19:55:44.669964 Epoch 106, Training loss 0.5320229924586423\n",
            "2022-12-07 19:55:57.175056 Epoch 107, Training loss 0.5303418970931216\n",
            "2022-12-07 19:56:09.500731 Epoch 108, Training loss 0.5282336606279664\n",
            "2022-12-07 19:56:22.192135 Epoch 109, Training loss 0.5265816047292231\n",
            "2022-12-07 19:56:34.662131 Epoch 110, Training loss 0.5254729456075317\n",
            "2022-12-07 19:56:47.144511 Epoch 111, Training loss 0.5230792596783784\n",
            "2022-12-07 19:56:59.649689 Epoch 112, Training loss 0.5230139505375376\n",
            "2022-12-07 19:57:12.025832 Epoch 113, Training loss 0.5198773747629217\n",
            "2022-12-07 19:57:24.572839 Epoch 114, Training loss 0.5209291373448603\n",
            "2022-12-07 19:57:36.880041 Epoch 115, Training loss 0.5177828144196355\n",
            "2022-12-07 19:57:49.333102 Epoch 116, Training loss 0.517027993622186\n",
            "2022-12-07 19:58:01.768602 Epoch 117, Training loss 0.5135484387350204\n",
            "2022-12-07 19:58:14.240789 Epoch 118, Training loss 0.5148930639371543\n",
            "2022-12-07 19:58:26.662124 Epoch 119, Training loss 0.5122780708019691\n",
            "2022-12-07 19:58:39.075898 Epoch 120, Training loss 0.5083928657576556\n",
            "2022-12-07 19:58:51.670338 Epoch 121, Training loss 0.5119139912831204\n",
            "2022-12-07 19:59:04.069246 Epoch 122, Training loss 0.5093713431902553\n",
            "2022-12-07 19:59:16.626074 Epoch 123, Training loss 0.5077904286934897\n",
            "2022-12-07 19:59:29.032207 Epoch 124, Training loss 0.5066875976407924\n",
            "2022-12-07 19:59:41.484953 Epoch 125, Training loss 0.5048942530856413\n",
            "2022-12-07 19:59:53.980839 Epoch 126, Training loss 0.5021087864170903\n",
            "2022-12-07 20:00:06.474446 Epoch 127, Training loss 0.5004171279004163\n",
            "2022-12-07 20:00:19.055744 Epoch 128, Training loss 0.5033560545205156\n",
            "2022-12-07 20:00:31.476165 Epoch 129, Training loss 0.49824194061329297\n",
            "2022-12-07 20:00:44.105183 Epoch 130, Training loss 0.49748818614446294\n",
            "2022-12-07 20:00:56.561946 Epoch 131, Training loss 0.49926098068351943\n",
            "2022-12-07 20:01:09.041026 Epoch 132, Training loss 0.4964253799727811\n",
            "2022-12-07 20:01:21.568765 Epoch 133, Training loss 0.49220317764126736\n",
            "2022-12-07 20:01:33.920346 Epoch 134, Training loss 0.4923246190561663\n",
            "2022-12-07 20:01:46.398831 Epoch 135, Training loss 0.4931675100418003\n",
            "2022-12-07 20:01:58.769132 Epoch 136, Training loss 0.4920162773498184\n",
            "2022-12-07 20:02:11.311140 Epoch 137, Training loss 0.4907944301891205\n",
            "2022-12-07 20:02:23.707928 Epoch 138, Training loss 0.4883727039522527\n",
            "2022-12-07 20:02:36.108015 Epoch 139, Training loss 0.48544697333937104\n",
            "2022-12-07 20:02:48.559681 Epoch 140, Training loss 0.48449744137427997\n",
            "2022-12-07 20:03:00.853009 Epoch 141, Training loss 0.48378552593614743\n",
            "2022-12-07 20:03:13.308381 Epoch 142, Training loss 0.4848367802589141\n",
            "2022-12-07 20:03:25.598626 Epoch 143, Training loss 0.4860654704444244\n",
            "2022-12-07 20:03:38.097892 Epoch 144, Training loss 0.48126417629020596\n",
            "2022-12-07 20:03:50.526563 Epoch 145, Training loss 0.48588009214843325\n",
            "2022-12-07 20:04:02.892808 Epoch 146, Training loss 0.4810912410354675\n",
            "2022-12-07 20:04:15.398684 Epoch 147, Training loss 0.47868642925530136\n",
            "2022-12-07 20:04:27.815971 Epoch 148, Training loss 0.4768135073640005\n",
            "2022-12-07 20:04:40.309841 Epoch 149, Training loss 0.4759734725708242\n",
            "2022-12-07 20:04:52.643016 Epoch 150, Training loss 0.47595403305328715\n",
            "2022-12-07 20:05:05.042039 Epoch 151, Training loss 0.4738924382897594\n",
            "2022-12-07 20:05:17.438422 Epoch 152, Training loss 0.47346667842486934\n",
            "2022-12-07 20:05:29.696142 Epoch 153, Training loss 0.47383341715311456\n",
            "2022-12-07 20:05:42.140251 Epoch 154, Training loss 0.47126576660767844\n",
            "2022-12-07 20:05:54.415206 Epoch 155, Training loss 0.47124125223482966\n",
            "2022-12-07 20:06:06.895935 Epoch 156, Training loss 0.4731503489910794\n",
            "2022-12-07 20:06:19.225988 Epoch 157, Training loss 0.4685779761361039\n",
            "2022-12-07 20:06:31.686284 Epoch 158, Training loss 0.4700203944197701\n",
            "2022-12-07 20:06:44.134461 Epoch 159, Training loss 0.46662808291595\n",
            "2022-12-07 20:06:56.497157 Epoch 160, Training loss 0.4642176858680632\n",
            "2022-12-07 20:07:09.029177 Epoch 161, Training loss 0.46600054585567824\n",
            "2022-12-07 20:07:21.335728 Epoch 162, Training loss 0.4653329110854422\n",
            "2022-12-07 20:07:33.876155 Epoch 163, Training loss 0.4632224892182728\n",
            "2022-12-07 20:07:46.290137 Epoch 164, Training loss 0.46061451815049664\n",
            "2022-12-07 20:07:58.769532 Epoch 165, Training loss 0.4596174474796066\n",
            "2022-12-07 20:08:11.280261 Epoch 166, Training loss 0.46153969117595106\n",
            "2022-12-07 20:08:23.694013 Epoch 167, Training loss 0.459610385834561\n",
            "2022-12-07 20:08:36.276231 Epoch 168, Training loss 0.46039700904465697\n",
            "2022-12-07 20:08:48.677448 Epoch 169, Training loss 0.45782895191856054\n",
            "2022-12-07 20:09:01.226755 Epoch 170, Training loss 0.4573184690245277\n",
            "2022-12-07 20:09:13.674634 Epoch 171, Training loss 0.45729989247858677\n",
            "2022-12-07 20:09:25.976654 Epoch 172, Training loss 0.4535183386920053\n",
            "2022-12-07 20:09:38.365796 Epoch 173, Training loss 0.4542918699743498\n",
            "2022-12-07 20:09:50.807936 Epoch 174, Training loss 0.45560047849822227\n",
            "2022-12-07 20:10:03.169423 Epoch 175, Training loss 0.4540001960552257\n",
            "2022-12-07 20:10:15.637218 Epoch 176, Training loss 0.4550704275593733\n",
            "2022-12-07 20:10:28.022170 Epoch 177, Training loss 0.45206676232997717\n",
            "2022-12-07 20:10:40.385937 Epoch 178, Training loss 0.4518741668604524\n",
            "2022-12-07 20:10:52.868197 Epoch 179, Training loss 0.4485720577256759\n",
            "2022-12-07 20:11:05.478460 Epoch 180, Training loss 0.44858937224616174\n",
            "2022-12-07 20:11:18.244769 Epoch 181, Training loss 0.4475553954958611\n",
            "2022-12-07 20:11:30.905630 Epoch 182, Training loss 0.4481647510052947\n",
            "2022-12-07 20:11:43.629556 Epoch 183, Training loss 0.44667585811499133\n",
            "2022-12-07 20:11:56.296175 Epoch 184, Training loss 0.4468887316258362\n",
            "2022-12-07 20:12:09.008626 Epoch 185, Training loss 0.4448605036872732\n",
            "2022-12-07 20:12:21.797664 Epoch 186, Training loss 0.44516640089814313\n",
            "2022-12-07 20:12:34.459653 Epoch 187, Training loss 0.44500250748508724\n",
            "2022-12-07 20:12:47.190114 Epoch 188, Training loss 0.4445114568866732\n",
            "2022-12-07 20:12:59.843245 Epoch 189, Training loss 0.44228042109543103\n",
            "2022-12-07 20:13:12.540714 Epoch 190, Training loss 0.4415316585346561\n",
            "2022-12-07 20:13:25.148184 Epoch 191, Training loss 0.44304856339760146\n",
            "2022-12-07 20:13:37.786545 Epoch 192, Training loss 0.4377110831610992\n",
            "2022-12-07 20:13:50.367144 Epoch 193, Training loss 0.441175075690917\n",
            "2022-12-07 20:14:03.086596 Epoch 194, Training loss 0.43797684737178677\n",
            "2022-12-07 20:14:15.726670 Epoch 195, Training loss 0.4392413278979719\n",
            "2022-12-07 20:14:28.412144 Epoch 196, Training loss 0.43717920974544855\n",
            "2022-12-07 20:14:41.120392 Epoch 197, Training loss 0.43718879509841085\n",
            "2022-12-07 20:14:53.871963 Epoch 198, Training loss 0.4353473202499282\n",
            "2022-12-07 20:15:06.622509 Epoch 199, Training loss 0.436150413930721\n",
            "2022-12-07 20:15:19.378540 Epoch 200, Training loss 0.4348466005700324\n",
            "2022-12-07 20:15:32.108856 Epoch 201, Training loss 0.4349547726152193\n",
            "2022-12-07 20:15:44.786205 Epoch 202, Training loss 0.4312520793941625\n",
            "2022-12-07 20:15:57.327018 Epoch 203, Training loss 0.42948329014241543\n",
            "2022-12-07 20:16:09.862278 Epoch 204, Training loss 0.4324784713518589\n",
            "2022-12-07 20:16:22.653602 Epoch 205, Training loss 0.43009636301518706\n",
            "2022-12-07 20:16:35.366037 Epoch 206, Training loss 0.4294077713433129\n",
            "2022-12-07 20:16:48.009505 Epoch 207, Training loss 0.4295608203315064\n",
            "2022-12-07 20:17:00.544834 Epoch 208, Training loss 0.43097188710556616\n",
            "2022-12-07 20:17:13.114965 Epoch 209, Training loss 0.4284425615845129\n",
            "2022-12-07 20:17:25.624110 Epoch 210, Training loss 0.429059560656014\n",
            "2022-12-07 20:17:38.096797 Epoch 211, Training loss 0.4263799125161927\n",
            "2022-12-07 20:17:50.496843 Epoch 212, Training loss 0.4251364374633335\n",
            "2022-12-07 20:18:02.778516 Epoch 213, Training loss 0.4248624210581755\n",
            "2022-12-07 20:18:15.080121 Epoch 214, Training loss 0.4283583523977138\n",
            "2022-12-07 20:18:27.309201 Epoch 215, Training loss 0.42869770854635314\n",
            "2022-12-07 20:18:39.562919 Epoch 216, Training loss 0.42407949543212686\n",
            "2022-12-07 20:18:51.925988 Epoch 217, Training loss 0.4252028497855377\n",
            "2022-12-07 20:19:04.251335 Epoch 218, Training loss 0.42245438837867866\n",
            "2022-12-07 20:19:16.573172 Epoch 219, Training loss 0.42419964690571244\n",
            "2022-12-07 20:19:28.901546 Epoch 220, Training loss 0.4243408190777235\n",
            "2022-12-07 20:19:41.286925 Epoch 221, Training loss 0.4241228372697025\n",
            "2022-12-07 20:19:53.567456 Epoch 222, Training loss 0.42170690423082513\n",
            "2022-12-07 20:20:05.915389 Epoch 223, Training loss 0.41948972145081176\n",
            "2022-12-07 20:20:18.300959 Epoch 224, Training loss 0.4199431514191201\n",
            "2022-12-07 20:20:30.674468 Epoch 225, Training loss 0.4192247179234424\n",
            "2022-12-07 20:20:43.077862 Epoch 226, Training loss 0.4212956242739697\n",
            "2022-12-07 20:20:55.445986 Epoch 227, Training loss 0.417827628190865\n",
            "2022-12-07 20:21:07.903417 Epoch 228, Training loss 0.41688906982579194\n",
            "2022-12-07 20:21:20.187568 Epoch 229, Training loss 0.41656286667679887\n",
            "2022-12-07 20:21:32.498095 Epoch 230, Training loss 0.41558762708359664\n",
            "2022-12-07 20:21:44.882141 Epoch 231, Training loss 0.41648235381640436\n",
            "2022-12-07 20:21:57.328987 Epoch 232, Training loss 0.4162466732399238\n",
            "2022-12-07 20:22:09.685960 Epoch 233, Training loss 0.4134770837967353\n",
            "2022-12-07 20:22:22.078016 Epoch 234, Training loss 0.41554746325211145\n",
            "2022-12-07 20:22:34.286722 Epoch 235, Training loss 0.41355534540036754\n",
            "2022-12-07 20:22:46.448668 Epoch 236, Training loss 0.4123773009556791\n",
            "2022-12-07 20:22:58.694178 Epoch 237, Training loss 0.4108624418678186\n",
            "2022-12-07 20:23:10.965243 Epoch 238, Training loss 0.41101447559531085\n",
            "2022-12-07 20:23:23.327326 Epoch 239, Training loss 0.411420085021983\n",
            "2022-12-07 20:23:35.735057 Epoch 240, Training loss 0.4088538463806252\n",
            "2022-12-07 20:23:48.120835 Epoch 241, Training loss 0.4092159063729179\n",
            "2022-12-07 20:24:00.488361 Epoch 242, Training loss 0.410240655112297\n",
            "2022-12-07 20:24:12.827125 Epoch 243, Training loss 0.4097167581624692\n",
            "2022-12-07 20:24:25.038450 Epoch 244, Training loss 0.4061169386900904\n",
            "2022-12-07 20:24:37.309434 Epoch 245, Training loss 0.40847079387253815\n",
            "2022-12-07 20:24:49.655151 Epoch 246, Training loss 0.4067608280979154\n",
            "2022-12-07 20:25:01.993514 Epoch 247, Training loss 0.4094428635676346\n",
            "2022-12-07 20:25:14.361196 Epoch 248, Training loss 0.4069998490688441\n",
            "2022-12-07 20:25:26.669886 Epoch 249, Training loss 0.4067184859720033\n",
            "2022-12-07 20:25:39.028768 Epoch 250, Training loss 0.4063632392784214\n",
            "2022-12-07 20:25:51.383382 Epoch 251, Training loss 0.40753532911810425\n",
            "2022-12-07 20:26:03.731421 Epoch 252, Training loss 0.4051060315097689\n",
            "2022-12-07 20:26:15.918494 Epoch 253, Training loss 0.4031722271610099\n",
            "2022-12-07 20:26:28.408134 Epoch 254, Training loss 0.4033978729296828\n",
            "2022-12-07 20:26:40.929125 Epoch 255, Training loss 0.40429017576567655\n",
            "2022-12-07 20:26:53.243014 Epoch 256, Training loss 0.40112249598936045\n",
            "2022-12-07 20:27:05.549874 Epoch 257, Training loss 0.40067132726273574\n",
            "2022-12-07 20:27:17.950842 Epoch 258, Training loss 0.4001911886207893\n",
            "2022-12-07 20:27:30.270790 Epoch 259, Training loss 0.4011602224901204\n",
            "2022-12-07 20:27:42.648474 Epoch 260, Training loss 0.4015078693247207\n",
            "2022-12-07 20:27:54.993324 Epoch 261, Training loss 0.3994131033758983\n",
            "2022-12-07 20:28:07.245232 Epoch 262, Training loss 0.4009030013323745\n",
            "2022-12-07 20:28:19.536339 Epoch 263, Training loss 0.3956669494319145\n",
            "2022-12-07 20:28:31.896442 Epoch 264, Training loss 0.4015372961454684\n",
            "2022-12-07 20:28:44.183180 Epoch 265, Training loss 0.3978680467704678\n",
            "2022-12-07 20:28:56.395709 Epoch 266, Training loss 0.39723072766952805\n",
            "2022-12-07 20:29:08.652895 Epoch 267, Training loss 0.3976364770089574\n",
            "2022-12-07 20:29:20.920337 Epoch 268, Training loss 0.39898824087722834\n",
            "2022-12-07 20:29:33.156965 Epoch 269, Training loss 0.3931455252325291\n",
            "2022-12-07 20:29:45.392948 Epoch 270, Training loss 0.39502368985539504\n",
            "2022-12-07 20:29:57.642762 Epoch 271, Training loss 0.40059285310794934\n",
            "2022-12-07 20:30:09.922418 Epoch 272, Training loss 0.3952890237426514\n",
            "2022-12-07 20:30:22.213352 Epoch 273, Training loss 0.39431519027027634\n",
            "2022-12-07 20:30:34.554200 Epoch 274, Training loss 0.39340084608253617\n",
            "2022-12-07 20:30:46.949006 Epoch 275, Training loss 0.3937451553619121\n",
            "2022-12-07 20:30:59.232454 Epoch 276, Training loss 0.39397362756835835\n",
            "2022-12-07 20:31:11.590168 Epoch 277, Training loss 0.39217417944422767\n",
            "2022-12-07 20:31:23.874460 Epoch 278, Training loss 0.3883469897081785\n",
            "2022-12-07 20:31:36.144733 Epoch 279, Training loss 0.3921171936880597\n",
            "2022-12-07 20:31:48.446753 Epoch 280, Training loss 0.3896667900521432\n",
            "2022-12-07 20:32:00.933220 Epoch 281, Training loss 0.3939493274521035\n",
            "2022-12-07 20:32:13.247691 Epoch 282, Training loss 0.39158129227130917\n",
            "2022-12-07 20:32:25.578360 Epoch 283, Training loss 0.39260507725617466\n",
            "2022-12-07 20:32:37.935364 Epoch 284, Training loss 0.3867003544593406\n",
            "2022-12-07 20:32:50.217663 Epoch 285, Training loss 0.38821926051774597\n",
            "2022-12-07 20:33:02.452838 Epoch 286, Training loss 0.3878686848332358\n",
            "2022-12-07 20:33:14.837259 Epoch 287, Training loss 0.39037887235660385\n",
            "2022-12-07 20:33:27.338468 Epoch 288, Training loss 0.3891740790794573\n",
            "2022-12-07 20:33:39.702721 Epoch 289, Training loss 0.3861888904324578\n",
            "2022-12-07 20:33:52.077524 Epoch 290, Training loss 0.38957135112541713\n",
            "2022-12-07 20:34:04.464714 Epoch 291, Training loss 0.3869743009986323\n",
            "2022-12-07 20:34:16.908278 Epoch 292, Training loss 0.38613296254440344\n",
            "2022-12-07 20:34:29.240249 Epoch 293, Training loss 0.38488042709010334\n",
            "2022-12-07 20:34:41.616257 Epoch 294, Training loss 0.38512707474972585\n",
            "2022-12-07 20:34:53.943756 Epoch 295, Training loss 0.3816826119733131\n",
            "2022-12-07 20:35:06.271016 Epoch 296, Training loss 0.3858716749512326\n",
            "2022-12-07 20:35:18.625615 Epoch 297, Training loss 0.3880976792567831\n",
            "2022-12-07 20:35:30.963117 Epoch 298, Training loss 0.38322474582649557\n",
            "2022-12-07 20:35:43.350950 Epoch 299, Training loss 0.383321490662787\n",
            "2022-12-07 20:35:55.717825 Epoch 300, Training loss 0.38513150704486293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                          shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                        shuffle=False)\n",
        "validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxYakBTOTZz-",
        "outputId": "226e80f4-7e67-489f-b91f-08e5c48936e3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.84\n",
            "Accuracy val: 0.68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part B, normalize batch conv layer"
      ],
      "metadata": {
        "id": "ABR4uYYCToVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NetRes(nn.Module):\n",
        "    def __init__(self, n_chans1=32):\n",
        "          super(NetRes, self).__init__()\n",
        "          self.conv = nn.Conv2d(n_chans1, n_chans1, \n",
        "            kernel_size=3, padding=1, bias=False)\n",
        "          self.batch_norm = nn.BatchNorm2d(num_features=\n",
        "                                     n_chans1)\n",
        "          torch.nn.init.kaiming_normal_(self.conv.weight, \n",
        "                            nonlinearity='relu')\n",
        "          torch.nn.init.constant_(self.batch_norm.weight, \n",
        "                            0.5)\n",
        "          torch.nn.init.zeros_(self.batch_norm.bias)\n",
        "        \n",
        "    def forward(self, x):\n",
        "     out = self.conv(x)\n",
        "     out = torch.relu(out)\n",
        "     return out + x\n"
      ],
      "metadata": {
        "id": "wHqCA11iT0xx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, \n",
        "                        batch_size=64, shuffle=True)\n",
        "\n",
        "model = Net().to('cuda:0')\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs =300,\n",
        "    optimizer= optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        "    \n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yj5favD0T3qt",
        "outputId": "637b2c88-b1df-4497-e6cf-f81ddb17bf75"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-12-07 20:38:29.457445 Epoch 1, Training loss 2.0330074699333562\n",
            "2022-12-07 20:38:41.250285 Epoch 2, Training loss 1.730403132450855\n",
            "2022-12-07 20:38:53.138689 Epoch 3, Training loss 1.5619641362553667\n",
            "2022-12-07 20:39:04.994569 Epoch 4, Training loss 1.468449181607922\n",
            "2022-12-07 20:39:16.809624 Epoch 5, Training loss 1.396753173807393\n",
            "2022-12-07 20:39:28.565919 Epoch 6, Training loss 1.3324053040550798\n",
            "2022-12-07 20:39:40.348994 Epoch 7, Training loss 1.2726079954972962\n",
            "2022-12-07 20:39:52.166385 Epoch 8, Training loss 1.2255400512224572\n",
            "2022-12-07 20:40:04.017866 Epoch 9, Training loss 1.188900394970194\n",
            "2022-12-07 20:40:15.836572 Epoch 10, Training loss 1.1564261196062082\n",
            "2022-12-07 20:40:27.665418 Epoch 11, Training loss 1.1301433671923244\n",
            "2022-12-07 20:40:39.464533 Epoch 12, Training loss 1.1069394989544168\n",
            "2022-12-07 20:40:51.313287 Epoch 13, Training loss 1.085099760681162\n",
            "2022-12-07 20:41:03.135575 Epoch 14, Training loss 1.0665156776490419\n",
            "2022-12-07 20:41:14.981731 Epoch 15, Training loss 1.046233906541639\n",
            "2022-12-07 20:41:26.777039 Epoch 16, Training loss 1.0279414303924725\n",
            "2022-12-07 20:41:38.577697 Epoch 17, Training loss 1.0131967034943574\n",
            "2022-12-07 20:41:50.421798 Epoch 18, Training loss 0.9992277607740954\n",
            "2022-12-07 20:42:02.203356 Epoch 19, Training loss 0.9857676058931424\n",
            "2022-12-07 20:42:13.953057 Epoch 20, Training loss 0.9734469634645125\n",
            "2022-12-07 20:42:25.792933 Epoch 21, Training loss 0.9626002249967717\n",
            "2022-12-07 20:42:37.507575 Epoch 22, Training loss 0.9526648307242966\n",
            "2022-12-07 20:42:49.324021 Epoch 23, Training loss 0.9405799667396204\n",
            "2022-12-07 20:43:01.114860 Epoch 24, Training loss 0.9323127370356293\n",
            "2022-12-07 20:43:13.007025 Epoch 25, Training loss 0.9266399509461639\n",
            "2022-12-07 20:43:24.908675 Epoch 26, Training loss 0.9186091035070931\n",
            "2022-12-07 20:43:36.778425 Epoch 27, Training loss 0.9111512657778952\n",
            "2022-12-07 20:43:48.631716 Epoch 28, Training loss 0.9037785994274842\n",
            "2022-12-07 20:44:00.523610 Epoch 29, Training loss 0.8974712166334967\n",
            "2022-12-07 20:44:12.354733 Epoch 30, Training loss 0.891523987359708\n",
            "2022-12-07 20:44:24.203303 Epoch 31, Training loss 0.885735024500381\n",
            "2022-12-07 20:44:36.036740 Epoch 32, Training loss 0.8800398371256221\n",
            "2022-12-07 20:44:47.866275 Epoch 33, Training loss 0.8745023340672788\n",
            "2022-12-07 20:44:59.727506 Epoch 34, Training loss 0.8684195730539844\n",
            "2022-12-07 20:45:11.581168 Epoch 35, Training loss 0.8651408146866753\n",
            "2022-12-07 20:45:23.436420 Epoch 36, Training loss 0.861077419708452\n",
            "2022-12-07 20:45:35.257820 Epoch 37, Training loss 0.8547961848318729\n",
            "2022-12-07 20:45:47.159241 Epoch 38, Training loss 0.8502118549764613\n",
            "2022-12-07 20:45:58.968276 Epoch 39, Training loss 0.8453670704684904\n",
            "2022-12-07 20:46:10.829682 Epoch 40, Training loss 0.8441672508826341\n",
            "2022-12-07 20:46:22.683319 Epoch 41, Training loss 0.8384800390209384\n",
            "2022-12-07 20:46:34.508657 Epoch 42, Training loss 0.8336625579373002\n",
            "2022-12-07 20:46:46.451420 Epoch 43, Training loss 0.830624770820903\n",
            "2022-12-07 20:46:58.339919 Epoch 44, Training loss 0.8254636752483485\n",
            "2022-12-07 20:47:10.175929 Epoch 45, Training loss 0.8226112459245545\n",
            "2022-12-07 20:47:21.969086 Epoch 46, Training loss 0.8182715540346892\n",
            "2022-12-07 20:47:33.792728 Epoch 47, Training loss 0.8148721286936489\n",
            "2022-12-07 20:47:45.642652 Epoch 48, Training loss 0.8108549730475906\n",
            "2022-12-07 20:47:57.509720 Epoch 49, Training loss 0.8070857961997961\n",
            "2022-12-07 20:48:09.359904 Epoch 50, Training loss 0.8043167990491823\n",
            "2022-12-07 20:48:21.212386 Epoch 51, Training loss 0.800687573564327\n",
            "2022-12-07 20:48:33.073262 Epoch 52, Training loss 0.7965252260341669\n",
            "2022-12-07 20:48:44.938646 Epoch 53, Training loss 0.793489649663191\n",
            "2022-12-07 20:48:56.952125 Epoch 54, Training loss 0.7919437976749352\n",
            "2022-12-07 20:49:08.722916 Epoch 55, Training loss 0.7867595640289814\n",
            "2022-12-07 20:49:20.489371 Epoch 56, Training loss 0.785228131715294\n",
            "2022-12-07 20:49:32.342498 Epoch 57, Training loss 0.780686726198172\n",
            "2022-12-07 20:49:44.184151 Epoch 58, Training loss 0.7797212631196317\n",
            "2022-12-07 20:49:56.095914 Epoch 59, Training loss 0.7758727013455022\n",
            "2022-12-07 20:50:08.194344 Epoch 60, Training loss 0.7728696002832154\n",
            "2022-12-07 20:50:20.051542 Epoch 61, Training loss 0.7706296984725596\n",
            "2022-12-07 20:50:31.907230 Epoch 62, Training loss 0.7668552003858035\n",
            "2022-12-07 20:50:43.821936 Epoch 63, Training loss 0.7658560083955145\n",
            "2022-12-07 20:50:55.658286 Epoch 64, Training loss 0.7621797632683268\n",
            "2022-12-07 20:51:07.536212 Epoch 65, Training loss 0.7584936589841038\n",
            "2022-12-07 20:51:19.352618 Epoch 66, Training loss 0.7566189196560998\n",
            "2022-12-07 20:51:31.161951 Epoch 67, Training loss 0.7539554725370139\n",
            "2022-12-07 20:51:42.984205 Epoch 68, Training loss 0.7518117647722858\n",
            "2022-12-07 20:51:54.804633 Epoch 69, Training loss 0.7496791915286838\n",
            "2022-12-07 20:52:06.645257 Epoch 70, Training loss 0.746879492117011\n",
            "2022-12-07 20:52:18.437423 Epoch 71, Training loss 0.7450285872916127\n",
            "2022-12-07 20:52:30.220958 Epoch 72, Training loss 0.7435659286387436\n",
            "2022-12-07 20:52:42.085006 Epoch 73, Training loss 0.7397061762068887\n",
            "2022-12-07 20:52:53.872675 Epoch 74, Training loss 0.7366888249850334\n",
            "2022-12-07 20:53:05.734625 Epoch 75, Training loss 0.7352208300014896\n",
            "2022-12-07 20:53:17.524377 Epoch 76, Training loss 0.733339919877784\n",
            "2022-12-07 20:53:29.473049 Epoch 77, Training loss 0.7299496568072482\n",
            "2022-12-07 20:53:41.314897 Epoch 78, Training loss 0.72783763893425\n",
            "2022-12-07 20:53:53.151492 Epoch 79, Training loss 0.727459062281472\n",
            "2022-12-07 20:54:05.046714 Epoch 80, Training loss 0.723821335436438\n",
            "2022-12-07 20:54:16.920196 Epoch 81, Training loss 0.7228647317651593\n",
            "2022-12-07 20:54:28.949613 Epoch 82, Training loss 0.7199501500410193\n",
            "2022-12-07 20:54:40.878042 Epoch 83, Training loss 0.7178321667491933\n",
            "2022-12-07 20:54:53.213833 Epoch 84, Training loss 0.7163107801428841\n",
            "2022-12-07 20:55:05.565066 Epoch 85, Training loss 0.7134544571952137\n",
            "2022-12-07 20:55:18.118302 Epoch 86, Training loss 0.7118864118519341\n",
            "2022-12-07 20:55:30.274919 Epoch 87, Training loss 0.7097564410141972\n",
            "2022-12-07 20:55:42.130110 Epoch 88, Training loss 0.7078750059961358\n",
            "2022-12-07 20:55:53.998875 Epoch 89, Training loss 0.7053412184157335\n",
            "2022-12-07 20:56:06.864428 Epoch 90, Training loss 0.7045562357625084\n",
            "2022-12-07 20:56:20.785996 Epoch 91, Training loss 0.7027147810553651\n",
            "2022-12-07 20:56:34.292366 Epoch 92, Training loss 0.7002541786035918\n",
            "2022-12-07 20:56:46.147295 Epoch 93, Training loss 0.6987595575697282\n",
            "2022-12-07 20:56:58.082322 Epoch 94, Training loss 0.6960546451303965\n",
            "2022-12-07 20:57:10.001377 Epoch 95, Training loss 0.6937436561678987\n",
            "2022-12-07 20:57:22.078766 Epoch 96, Training loss 0.6934976907413634\n",
            "2022-12-07 20:57:34.003741 Epoch 97, Training loss 0.6905245609448084\n",
            "2022-12-07 20:57:45.829281 Epoch 98, Training loss 0.6892763572504453\n",
            "2022-12-07 20:57:57.735096 Epoch 99, Training loss 0.6872543113692032\n",
            "2022-12-07 20:58:09.563664 Epoch 100, Training loss 0.6866605831000506\n",
            "2022-12-07 20:58:21.375040 Epoch 101, Training loss 0.6849777641351266\n",
            "2022-12-07 20:58:33.288516 Epoch 102, Training loss 0.6817452019209143\n",
            "2022-12-07 20:58:45.227776 Epoch 103, Training loss 0.6816558969752563\n",
            "2022-12-07 20:58:57.138747 Epoch 104, Training loss 0.6793126408630015\n",
            "2022-12-07 20:59:09.066669 Epoch 105, Training loss 0.6770670038202534\n",
            "2022-12-07 20:59:20.949257 Epoch 106, Training loss 0.6746237663661733\n",
            "2022-12-07 20:59:32.783645 Epoch 107, Training loss 0.6752956564088002\n",
            "2022-12-07 20:59:44.669574 Epoch 108, Training loss 0.6732112455474751\n",
            "2022-12-07 20:59:56.462865 Epoch 109, Training loss 0.6709983998628528\n",
            "2022-12-07 21:00:08.355060 Epoch 110, Training loss 0.6708429187841123\n",
            "2022-12-07 21:00:20.210578 Epoch 111, Training loss 0.6672221293763432\n",
            "2022-12-07 21:00:32.089663 Epoch 112, Training loss 0.666036792804518\n",
            "2022-12-07 21:00:43.894272 Epoch 113, Training loss 0.6655047845352641\n",
            "2022-12-07 21:00:55.810649 Epoch 114, Training loss 0.665102623269686\n",
            "2022-12-07 21:01:07.796258 Epoch 115, Training loss 0.6632995550208689\n",
            "2022-12-07 21:01:19.721328 Epoch 116, Training loss 0.6636331134744923\n",
            "2022-12-07 21:01:31.554307 Epoch 117, Training loss 0.6606358184915064\n",
            "2022-12-07 21:01:43.407058 Epoch 118, Training loss 0.6587913191836813\n",
            "2022-12-07 21:01:55.258455 Epoch 119, Training loss 0.6558831727413266\n",
            "2022-12-07 21:02:07.131655 Epoch 120, Training loss 0.6562468380193271\n",
            "2022-12-07 21:02:19.119692 Epoch 121, Training loss 0.6543675175941813\n",
            "2022-12-07 21:02:31.059651 Epoch 122, Training loss 0.6528023062154765\n",
            "2022-12-07 21:02:43.058932 Epoch 123, Training loss 0.6512687030960532\n",
            "2022-12-07 21:02:55.017468 Epoch 124, Training loss 0.6515642907232275\n",
            "2022-12-07 21:03:07.084472 Epoch 125, Training loss 0.6489916713646305\n",
            "2022-12-07 21:03:19.134377 Epoch 126, Training loss 0.6482809855962348\n",
            "2022-12-07 21:03:31.124369 Epoch 127, Training loss 0.6463173743327866\n",
            "2022-12-07 21:03:42.970184 Epoch 128, Training loss 0.6450751580850548\n",
            "2022-12-07 21:03:54.778893 Epoch 129, Training loss 0.6441231789186482\n",
            "2022-12-07 21:04:06.588226 Epoch 130, Training loss 0.6432446434979548\n",
            "2022-12-07 21:04:18.376904 Epoch 131, Training loss 0.6414409467326406\n",
            "2022-12-07 21:04:30.188412 Epoch 132, Training loss 0.6416147878712706\n",
            "2022-12-07 21:04:41.991567 Epoch 133, Training loss 0.6388028802926583\n",
            "2022-12-07 21:04:53.757545 Epoch 134, Training loss 0.6369962027615599\n",
            "2022-12-07 21:05:05.587779 Epoch 135, Training loss 0.6358383663017732\n",
            "2022-12-07 21:05:17.415123 Epoch 136, Training loss 0.636094559412783\n",
            "2022-12-07 21:05:29.199710 Epoch 137, Training loss 0.6347536843298646\n",
            "2022-12-07 21:05:41.118960 Epoch 138, Training loss 0.6328091749830929\n",
            "2022-12-07 21:05:53.118850 Epoch 139, Training loss 0.6321992386713662\n",
            "2022-12-07 21:06:05.146855 Epoch 140, Training loss 0.631741655833276\n",
            "2022-12-07 21:06:17.125393 Epoch 141, Training loss 0.6294459208579319\n",
            "2022-12-07 21:06:29.121393 Epoch 142, Training loss 0.6287577839001365\n",
            "2022-12-07 21:06:41.115496 Epoch 143, Training loss 0.6257945552201527\n",
            "2022-12-07 21:06:53.094299 Epoch 144, Training loss 0.6266708702153867\n",
            "2022-12-07 21:07:05.069129 Epoch 145, Training loss 0.6245381403380953\n",
            "2022-12-07 21:07:17.079814 Epoch 146, Training loss 0.6249578537614754\n",
            "2022-12-07 21:07:29.051006 Epoch 147, Training loss 0.6237912793522296\n",
            "2022-12-07 21:07:41.042788 Epoch 148, Training loss 0.6211824424355231\n",
            "2022-12-07 21:07:52.877915 Epoch 149, Training loss 0.6213502893057625\n",
            "2022-12-07 21:08:04.729937 Epoch 150, Training loss 0.6196181579014225\n",
            "2022-12-07 21:08:16.576175 Epoch 151, Training loss 0.6183554656670222\n",
            "2022-12-07 21:08:28.450659 Epoch 152, Training loss 0.6166017603157731\n",
            "2022-12-07 21:08:40.284331 Epoch 153, Training loss 0.6166722926566058\n",
            "2022-12-07 21:08:52.189214 Epoch 154, Training loss 0.6156652998512663\n",
            "2022-12-07 21:09:04.028296 Epoch 155, Training loss 0.6139760159927866\n",
            "2022-12-07 21:09:15.916740 Epoch 156, Training loss 0.6139461018926348\n",
            "2022-12-07 21:09:27.921777 Epoch 157, Training loss 0.6119270900936078\n",
            "2022-12-07 21:09:39.948618 Epoch 158, Training loss 0.6119314243878855\n",
            "2022-12-07 21:09:51.846941 Epoch 159, Training loss 0.6111133744954453\n",
            "2022-12-07 21:10:03.714330 Epoch 160, Training loss 0.6089913802759727\n",
            "2022-12-07 21:10:15.588563 Epoch 161, Training loss 0.6081946460944613\n",
            "2022-12-07 21:10:27.487798 Epoch 162, Training loss 0.6074701131838361\n",
            "2022-12-07 21:10:39.206518 Epoch 163, Training loss 0.6056487430125246\n",
            "2022-12-07 21:10:51.095945 Epoch 164, Training loss 0.606740287274046\n",
            "2022-12-07 21:11:02.831459 Epoch 165, Training loss 0.6055935497021736\n",
            "2022-12-07 21:11:14.761422 Epoch 166, Training loss 0.603534705765412\n",
            "2022-12-07 21:11:26.602570 Epoch 167, Training loss 0.6034096140614555\n",
            "2022-12-07 21:11:38.399273 Epoch 168, Training loss 0.6021276773775325\n",
            "2022-12-07 21:11:50.249372 Epoch 169, Training loss 0.5998711969389026\n",
            "2022-12-07 21:12:02.057510 Epoch 170, Training loss 0.6006505202378154\n",
            "2022-12-07 21:12:13.769934 Epoch 171, Training loss 0.5981635338510088\n",
            "2022-12-07 21:12:25.553000 Epoch 172, Training loss 0.5984549966004803\n",
            "2022-12-07 21:12:37.336294 Epoch 173, Training loss 0.5971989715495682\n",
            "2022-12-07 21:12:49.185466 Epoch 174, Training loss 0.5958622029751462\n",
            "2022-12-07 21:13:00.994800 Epoch 175, Training loss 0.5962108377453006\n",
            "2022-12-07 21:13:12.849825 Epoch 176, Training loss 0.5947446215450002\n",
            "2022-12-07 21:13:24.636228 Epoch 177, Training loss 0.5937382809036528\n",
            "2022-12-07 21:13:36.478150 Epoch 178, Training loss 0.5925274346109546\n",
            "2022-12-07 21:13:48.334010 Epoch 179, Training loss 0.5932181067860035\n",
            "2022-12-07 21:14:00.156383 Epoch 180, Training loss 0.5899411161308703\n",
            "2022-12-07 21:14:11.933598 Epoch 181, Training loss 0.5913112919653774\n",
            "2022-12-07 21:14:23.750034 Epoch 182, Training loss 0.5901684279332076\n",
            "2022-12-07 21:14:35.488888 Epoch 183, Training loss 0.588674269140224\n",
            "2022-12-07 21:14:47.346309 Epoch 184, Training loss 0.5885683668543921\n",
            "2022-12-07 21:14:59.109914 Epoch 185, Training loss 0.586543163908717\n",
            "2022-12-07 21:15:10.929379 Epoch 186, Training loss 0.5853861323784074\n",
            "2022-12-07 21:15:22.756238 Epoch 187, Training loss 0.587332196133521\n",
            "2022-12-07 21:15:34.590368 Epoch 188, Training loss 0.5852554978998116\n",
            "2022-12-07 21:15:46.430390 Epoch 189, Training loss 0.5840890236827724\n",
            "2022-12-07 21:15:58.189739 Epoch 190, Training loss 0.5828372429856254\n",
            "2022-12-07 21:16:09.991599 Epoch 191, Training loss 0.5819609064199126\n",
            "2022-12-07 21:16:21.860384 Epoch 192, Training loss 0.5804437777346663\n",
            "2022-12-07 21:16:33.703118 Epoch 193, Training loss 0.5795299634909081\n",
            "2022-12-07 21:16:45.606823 Epoch 194, Training loss 0.5787059807640207\n",
            "2022-12-07 21:16:57.413241 Epoch 195, Training loss 0.5774214762403532\n",
            "2022-12-07 21:17:09.309255 Epoch 196, Training loss 0.5792446852187672\n",
            "2022-12-07 21:17:21.157633 Epoch 197, Training loss 0.5780630537768459\n",
            "2022-12-07 21:17:33.048512 Epoch 198, Training loss 0.5779818874567061\n",
            "2022-12-07 21:17:44.929418 Epoch 199, Training loss 0.5760322957849868\n",
            "2022-12-07 21:17:56.847501 Epoch 200, Training loss 0.5754042259033989\n",
            "2022-12-07 21:18:08.641658 Epoch 201, Training loss 0.5747115458826275\n",
            "2022-12-07 21:18:20.545232 Epoch 202, Training loss 0.5725636819896796\n",
            "2022-12-07 21:18:32.456369 Epoch 203, Training loss 0.572077308050202\n",
            "2022-12-07 21:18:44.369530 Epoch 204, Training loss 0.572491853247823\n",
            "2022-12-07 21:18:56.125463 Epoch 205, Training loss 0.5716884170880403\n",
            "2022-12-07 21:19:08.101225 Epoch 206, Training loss 0.5711938691947162\n",
            "2022-12-07 21:19:20.083270 Epoch 207, Training loss 0.5691543457376987\n",
            "2022-12-07 21:19:32.064921 Epoch 208, Training loss 0.5697276601401131\n",
            "2022-12-07 21:19:44.045427 Epoch 209, Training loss 0.5682429655281174\n",
            "2022-12-07 21:19:56.032737 Epoch 210, Training loss 0.5695954158787837\n",
            "2022-12-07 21:20:08.113950 Epoch 211, Training loss 0.5685118310286871\n",
            "2022-12-07 21:20:20.098476 Epoch 212, Training loss 0.565609326333646\n",
            "2022-12-07 21:20:32.175350 Epoch 213, Training loss 0.566910528770798\n",
            "2022-12-07 21:20:44.141077 Epoch 214, Training loss 0.5654481556028357\n",
            "2022-12-07 21:20:56.103015 Epoch 215, Training loss 0.563853169493663\n",
            "2022-12-07 21:21:08.189478 Epoch 216, Training loss 0.5641312303445528\n",
            "2022-12-07 21:21:20.226285 Epoch 217, Training loss 0.5618656233448507\n",
            "2022-12-07 21:21:32.276944 Epoch 218, Training loss 0.5617734475818741\n",
            "2022-12-07 21:21:44.357388 Epoch 219, Training loss 0.5606260960135618\n",
            "2022-12-07 21:21:56.325400 Epoch 220, Training loss 0.5613224843655096\n",
            "2022-12-07 21:22:08.298883 Epoch 221, Training loss 0.5625065433628419\n",
            "2022-12-07 21:22:20.188999 Epoch 222, Training loss 0.5583536424447814\n",
            "2022-12-07 21:22:32.203238 Epoch 223, Training loss 0.5589931568754908\n",
            "2022-12-07 21:22:44.349392 Epoch 224, Training loss 0.5591311021838956\n",
            "2022-12-07 21:22:56.340578 Epoch 225, Training loss 0.5585197347890386\n",
            "2022-12-07 21:23:08.319793 Epoch 226, Training loss 0.5581709059989056\n",
            "2022-12-07 21:23:20.331786 Epoch 227, Training loss 0.5569763215606475\n",
            "2022-12-07 21:23:32.293527 Epoch 228, Training loss 0.554520386830925\n",
            "2022-12-07 21:23:44.465158 Epoch 229, Training loss 0.5559347071077513\n",
            "2022-12-07 21:23:56.505594 Epoch 230, Training loss 0.5559726576213642\n",
            "2022-12-07 21:24:08.634483 Epoch 231, Training loss 0.5550558111437446\n",
            "2022-12-07 21:24:20.760798 Epoch 232, Training loss 0.5536428694148807\n",
            "2022-12-07 21:24:32.764463 Epoch 233, Training loss 0.5525112921548316\n",
            "2022-12-07 21:24:44.860788 Epoch 234, Training loss 0.5521616351116648\n",
            "2022-12-07 21:24:57.035914 Epoch 235, Training loss 0.5517806016728092\n",
            "2022-12-07 21:25:09.163997 Epoch 236, Training loss 0.5517918660741328\n",
            "2022-12-07 21:25:21.236885 Epoch 237, Training loss 0.5519378037403917\n",
            "2022-12-07 21:25:33.250771 Epoch 238, Training loss 0.5488057997068176\n",
            "2022-12-07 21:25:45.428139 Epoch 239, Training loss 0.550602380691282\n",
            "2022-12-07 21:25:57.534333 Epoch 240, Training loss 0.5484665747341293\n",
            "2022-12-07 21:26:09.547354 Epoch 241, Training loss 0.547583168958459\n",
            "2022-12-07 21:26:21.565674 Epoch 242, Training loss 0.5468371219342322\n",
            "2022-12-07 21:26:33.448371 Epoch 243, Training loss 0.5454358547316183\n",
            "2022-12-07 21:26:45.417713 Epoch 244, Training loss 0.5475632280987852\n",
            "2022-12-07 21:26:57.536991 Epoch 245, Training loss 0.5464466609384703\n",
            "2022-12-07 21:27:09.776021 Epoch 246, Training loss 0.5449536961057911\n",
            "2022-12-07 21:27:21.823666 Epoch 247, Training loss 0.545004846894985\n",
            "2022-12-07 21:27:33.873734 Epoch 248, Training loss 0.5449782645382235\n",
            "2022-12-07 21:27:45.914425 Epoch 249, Training loss 0.5425195388133873\n",
            "2022-12-07 21:27:58.024006 Epoch 250, Training loss 0.5425293061815565\n",
            "2022-12-07 21:28:10.168478 Epoch 251, Training loss 0.5435893262362541\n",
            "2022-12-07 21:28:22.193170 Epoch 252, Training loss 0.5423060150631248\n",
            "2022-12-07 21:28:34.251135 Epoch 253, Training loss 0.5416407520546938\n",
            "2022-12-07 21:28:46.338169 Epoch 254, Training loss 0.5406436114512441\n",
            "2022-12-07 21:28:58.441482 Epoch 255, Training loss 0.5418240251138692\n",
            "2022-12-07 21:29:10.524068 Epoch 256, Training loss 0.5408689332435198\n",
            "2022-12-07 21:29:22.599877 Epoch 257, Training loss 0.5407166031696607\n",
            "2022-12-07 21:29:34.685854 Epoch 258, Training loss 0.5399345150955802\n",
            "2022-12-07 21:29:46.798513 Epoch 259, Training loss 0.5375720000137454\n",
            "2022-12-07 21:29:58.985627 Epoch 260, Training loss 0.5372100003311396\n",
            "2022-12-07 21:30:11.225403 Epoch 261, Training loss 0.536563859037731\n",
            "2022-12-07 21:30:23.433489 Epoch 262, Training loss 0.5372058484712829\n",
            "2022-12-07 21:30:35.574026 Epoch 263, Training loss 0.5375577640884063\n",
            "2022-12-07 21:30:47.669620 Epoch 264, Training loss 0.5360800946688713\n",
            "2022-12-07 21:30:59.846727 Epoch 265, Training loss 0.5369433955577634\n",
            "2022-12-07 21:31:11.910121 Epoch 266, Training loss 0.5355887353001043\n",
            "2022-12-07 21:31:24.090973 Epoch 267, Training loss 0.5348659801056318\n",
            "2022-12-07 21:31:36.212339 Epoch 268, Training loss 0.5344192779734921\n",
            "2022-12-07 21:31:48.254517 Epoch 269, Training loss 0.5334289486298476\n",
            "2022-12-07 21:32:00.174214 Epoch 270, Training loss 0.5330026461111615\n",
            "2022-12-07 21:32:12.130021 Epoch 271, Training loss 0.5316972859070429\n",
            "2022-12-07 21:32:24.252271 Epoch 272, Training loss 0.5308254456047512\n",
            "2022-12-07 21:32:36.345295 Epoch 273, Training loss 0.5330556758758053\n",
            "2022-12-07 21:32:48.313642 Epoch 274, Training loss 0.5322117324337325\n",
            "2022-12-07 21:33:00.355881 Epoch 275, Training loss 0.5305078076889448\n",
            "2022-12-07 21:33:12.481400 Epoch 276, Training loss 0.5298604989219504\n",
            "2022-12-07 21:33:24.558392 Epoch 277, Training loss 0.5309971533811001\n",
            "2022-12-07 21:33:36.712178 Epoch 278, Training loss 0.5289046658045801\n",
            "2022-12-07 21:33:48.890802 Epoch 279, Training loss 0.5278761652500733\n",
            "2022-12-07 21:34:00.909347 Epoch 280, Training loss 0.5285624317882006\n",
            "2022-12-07 21:34:12.996346 Epoch 281, Training loss 0.5275614140436168\n",
            "2022-12-07 21:34:25.062648 Epoch 282, Training loss 0.5274486019064093\n",
            "2022-12-07 21:34:37.099865 Epoch 283, Training loss 0.5278736659899697\n",
            "2022-12-07 21:34:49.205578 Epoch 284, Training loss 0.527933725367879\n",
            "2022-12-07 21:35:01.266193 Epoch 285, Training loss 0.5280546350857181\n",
            "2022-12-07 21:35:13.257927 Epoch 286, Training loss 0.5271644138771555\n",
            "2022-12-07 21:35:25.278088 Epoch 287, Training loss 0.5263856672455588\n",
            "2022-12-07 21:35:37.311815 Epoch 288, Training loss 0.5260330004917692\n",
            "2022-12-07 21:35:49.434210 Epoch 289, Training loss 0.5256969910639021\n",
            "2022-12-07 21:36:01.734318 Epoch 290, Training loss 0.5248934317122945\n",
            "2022-12-07 21:36:14.059531 Epoch 291, Training loss 0.5234674108226586\n",
            "2022-12-07 21:36:26.379525 Epoch 292, Training loss 0.5222599823266039\n",
            "2022-12-07 21:36:38.670074 Epoch 293, Training loss 0.5231912176665443\n",
            "2022-12-07 21:36:50.769256 Epoch 294, Training loss 0.5216740359506948\n",
            "2022-12-07 21:37:02.951620 Epoch 295, Training loss 0.5198882850615875\n",
            "2022-12-07 21:37:15.146343 Epoch 296, Training loss 0.5216606279735065\n",
            "2022-12-07 21:37:27.249475 Epoch 297, Training loss 0.521271392074235\n",
            "2022-12-07 21:37:39.255564 Epoch 298, Training loss 0.5215943800785657\n",
            "2022-12-07 21:37:51.284116 Epoch 299, Training loss 0.5218297844500188\n",
            "2022-12-07 21:38:03.339236 Epoch 300, Training loss 0.5190344316041683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                          shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                        shuffle=False)\n",
        "validate(model, train_loader, val_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vK0Cf2BRhnLn",
        "outputId": "8696ace3-fdbb-4f71-ac91-0ddcaf251c29"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.82\n",
            "Accuracy val: 0.62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part B Dropout Activation"
      ],
      "metadata": {
        "id": "MY8WWI2ohtUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NetRes(nn.Module):\n",
        "    def __init__(self, n_chans1=32, n_blocks=10):\n",
        "      super().__init__()\n",
        "      self.n_chans1 = n_chans1\n",
        "      self.conv1 = nn.Sequential(\n",
        "        *(n_blocks * [NetRes(n_chans1=n_chans1)]))\n",
        "      self.conv1_dropout = nn.Dropout2d(p=0.3)\n",
        "      self.fc1 = nn.Linear(8*8*n_chans1 // 2, 32)\n",
        "      self.fc2 = nn.Linear(32,2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "      out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "      out = self.conv1_dropout(out)\n",
        "      out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "      out = self.conv2_dropout(out)\n",
        "      out = out.view(-1, 8*8*self.n_chans1 // 2)\n",
        "      out = torch.tanh(self.fc1(out))\n",
        "      out = self.fc2(out)\n",
        "      return out"
      ],
      "metadata": {
        "id": "gjojnde1hyJ8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, \n",
        "                        batch_size=64, shuffle=True)\n",
        "\n",
        "model = Net().to('cuda:0')\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs =300,\n",
        "    optimizer= optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        "    \n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm7zay3ih1B5",
        "outputId": "b37207d7-ee61-40a7-ccca-eccbc1176306"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-12-07 21:39:34.725224 Epoch 1, Training loss 2.0523412044700757\n",
            "2022-12-07 21:39:46.803430 Epoch 2, Training loss 1.8290732533425627\n",
            "2022-12-07 21:39:58.869042 Epoch 3, Training loss 1.6545825513732402\n",
            "2022-12-07 21:40:11.089745 Epoch 4, Training loss 1.5343294533927117\n",
            "2022-12-07 21:40:23.203323 Epoch 5, Training loss 1.4583565065318056\n",
            "2022-12-07 21:40:35.283930 Epoch 6, Training loss 1.390545995491545\n",
            "2022-12-07 21:40:47.388342 Epoch 7, Training loss 1.3248967944508623\n",
            "2022-12-07 21:40:59.474130 Epoch 8, Training loss 1.2681890617093772\n",
            "2022-12-07 21:41:11.663181 Epoch 9, Training loss 1.225583272135776\n",
            "2022-12-07 21:41:23.729705 Epoch 10, Training loss 1.1904671711994863\n",
            "2022-12-07 21:41:35.827664 Epoch 11, Training loss 1.159213604677059\n",
            "2022-12-07 21:41:47.900320 Epoch 12, Training loss 1.1327191531048406\n",
            "2022-12-07 21:41:59.933491 Epoch 13, Training loss 1.108315679956885\n",
            "2022-12-07 21:42:12.079786 Epoch 14, Training loss 1.0868111956302466\n",
            "2022-12-07 21:42:24.146927 Epoch 15, Training loss 1.0658721392569335\n",
            "2022-12-07 21:42:36.274729 Epoch 16, Training loss 1.0479930745213843\n",
            "2022-12-07 21:42:48.230736 Epoch 17, Training loss 1.0310577207514087\n",
            "2022-12-07 21:43:00.293611 Epoch 18, Training loss 1.014735446180529\n",
            "2022-12-07 21:43:12.400262 Epoch 19, Training loss 0.999751745130095\n",
            "2022-12-07 21:43:24.465506 Epoch 20, Training loss 0.987384811310512\n",
            "2022-12-07 21:43:36.671491 Epoch 21, Training loss 0.9724579439748584\n",
            "2022-12-07 21:43:48.846572 Epoch 22, Training loss 0.9632605713651613\n",
            "2022-12-07 21:44:01.092457 Epoch 23, Training loss 0.9519723843583061\n",
            "2022-12-07 21:44:13.287083 Epoch 24, Training loss 0.9430702368316748\n",
            "2022-12-07 21:44:25.283407 Epoch 25, Training loss 0.9329966068115381\n",
            "2022-12-07 21:44:37.331908 Epoch 26, Training loss 0.9256728399745033\n",
            "2022-12-07 21:44:49.485108 Epoch 27, Training loss 0.9157513266481707\n",
            "2022-12-07 21:45:01.575541 Epoch 28, Training loss 0.9071448204462486\n",
            "2022-12-07 21:45:13.626715 Epoch 29, Training loss 0.9009754699666787\n",
            "2022-12-07 21:45:25.660079 Epoch 30, Training loss 0.8927948979465553\n",
            "2022-12-07 21:45:37.793519 Epoch 31, Training loss 0.8874983106122907\n",
            "2022-12-07 21:45:49.950634 Epoch 32, Training loss 0.8812897494229515\n",
            "2022-12-07 21:46:02.055216 Epoch 33, Training loss 0.8756232476600295\n",
            "2022-12-07 21:46:13.932812 Epoch 34, Training loss 0.8694213721758265\n",
            "2022-12-07 21:46:25.852228 Epoch 35, Training loss 0.8649116953468079\n",
            "2022-12-07 21:46:38.049337 Epoch 36, Training loss 0.8586526065683731\n",
            "2022-12-07 21:46:50.324180 Epoch 37, Training loss 0.8536705984483899\n",
            "2022-12-07 21:47:02.384488 Epoch 38, Training loss 0.8464592504303169\n",
            "2022-12-07 21:47:14.627956 Epoch 39, Training loss 0.844021548929117\n",
            "2022-12-07 21:47:26.757138 Epoch 40, Training loss 0.8398679357279292\n",
            "2022-12-07 21:47:38.849908 Epoch 41, Training loss 0.8350342754512796\n",
            "2022-12-07 21:47:50.930176 Epoch 42, Training loss 0.8288204615073436\n",
            "2022-12-07 21:48:03.022779 Epoch 43, Training loss 0.8266561694462281\n",
            "2022-12-07 21:48:15.166957 Epoch 44, Training loss 0.8225915683885975\n",
            "2022-12-07 21:48:27.274430 Epoch 45, Training loss 0.8181259578756054\n",
            "2022-12-07 21:48:39.320481 Epoch 46, Training loss 0.8145748725937455\n",
            "2022-12-07 21:48:51.438806 Epoch 47, Training loss 0.8102844445144429\n",
            "2022-12-07 21:49:03.520924 Epoch 48, Training loss 0.8063269471725845\n",
            "2022-12-07 21:49:15.648480 Epoch 49, Training loss 0.8031262530161597\n",
            "2022-12-07 21:49:27.795455 Epoch 50, Training loss 0.799414097386248\n",
            "2022-12-07 21:49:40.085744 Epoch 51, Training loss 0.7954315677323305\n",
            "2022-12-07 21:49:52.312246 Epoch 52, Training loss 0.7945072616228972\n",
            "2022-12-07 21:50:04.557091 Epoch 53, Training loss 0.7884413069852477\n",
            "2022-12-07 21:50:16.779744 Epoch 54, Training loss 0.7849303068940902\n",
            "2022-12-07 21:50:28.903077 Epoch 55, Training loss 0.7822776832772643\n",
            "2022-12-07 21:50:41.076908 Epoch 56, Training loss 0.7793543072384032\n",
            "2022-12-07 21:50:53.229026 Epoch 57, Training loss 0.7759364561732772\n",
            "2022-12-07 21:51:05.307222 Epoch 58, Training loss 0.7725401221181426\n",
            "2022-12-07 21:51:17.340943 Epoch 59, Training loss 0.7693528302795137\n",
            "2022-12-07 21:51:29.356954 Epoch 60, Training loss 0.767286055190179\n",
            "2022-12-07 21:51:41.470855 Epoch 61, Training loss 0.7640720295250568\n",
            "2022-12-07 21:51:53.427251 Epoch 62, Training loss 0.7614047874498855\n",
            "2022-12-07 21:52:05.419675 Epoch 63, Training loss 0.7580267902454147\n",
            "2022-12-07 21:52:17.368440 Epoch 64, Training loss 0.7564019138550819\n",
            "2022-12-07 21:52:29.354451 Epoch 65, Training loss 0.7537523130397967\n",
            "2022-12-07 21:52:41.392097 Epoch 66, Training loss 0.7505909116066936\n",
            "2022-12-07 21:52:53.400546 Epoch 67, Training loss 0.7473923304806585\n",
            "2022-12-07 21:53:05.353322 Epoch 68, Training loss 0.7463577208692765\n",
            "2022-12-07 21:53:17.445906 Epoch 69, Training loss 0.7429300247860686\n",
            "2022-12-07 21:53:29.444894 Epoch 70, Training loss 0.7391047373680812\n",
            "2022-12-07 21:53:41.452390 Epoch 71, Training loss 0.7380323155075693\n",
            "2022-12-07 21:53:53.485411 Epoch 72, Training loss 0.73336954266214\n",
            "2022-12-07 21:54:05.435450 Epoch 73, Training loss 0.7339007341114762\n",
            "2022-12-07 21:54:17.374520 Epoch 74, Training loss 0.7313774404547099\n",
            "2022-12-07 21:54:29.317491 Epoch 75, Training loss 0.7285412076260428\n",
            "2022-12-07 21:54:41.275124 Epoch 76, Training loss 0.7268256874340574\n",
            "2022-12-07 21:54:53.264928 Epoch 77, Training loss 0.7238601355449014\n",
            "2022-12-07 21:55:05.179104 Epoch 78, Training loss 0.7213617276276469\n",
            "2022-12-07 21:55:17.138830 Epoch 79, Training loss 0.7203894849018673\n",
            "2022-12-07 21:55:29.362391 Epoch 80, Training loss 0.7177523682870524\n",
            "2022-12-07 21:55:41.404475 Epoch 81, Training loss 0.7146057958340706\n",
            "2022-12-07 21:55:53.425440 Epoch 82, Training loss 0.7130551423563067\n",
            "2022-12-07 21:56:05.367817 Epoch 83, Training loss 0.7112279551870683\n",
            "2022-12-07 21:56:17.305778 Epoch 84, Training loss 0.7084281656443311\n",
            "2022-12-07 21:56:29.291375 Epoch 85, Training loss 0.7049429079760676\n",
            "2022-12-07 21:56:41.381155 Epoch 86, Training loss 0.705356073615801\n",
            "2022-12-07 21:56:53.324331 Epoch 87, Training loss 0.7023988395853116\n",
            "2022-12-07 21:57:05.394043 Epoch 88, Training loss 0.6999531962606304\n",
            "2022-12-07 21:57:17.499967 Epoch 89, Training loss 0.698733811297685\n",
            "2022-12-07 21:57:29.592803 Epoch 90, Training loss 0.6976266345938148\n",
            "2022-12-07 21:57:41.573737 Epoch 91, Training loss 0.6945674974290307\n",
            "2022-12-07 21:57:53.569476 Epoch 92, Training loss 0.6930704893892073\n",
            "2022-12-07 21:58:05.627912 Epoch 93, Training loss 0.6901455108466965\n",
            "2022-12-07 21:58:17.699475 Epoch 94, Training loss 0.6887927632731246\n",
            "2022-12-07 21:58:29.690633 Epoch 95, Training loss 0.6879954811404733\n",
            "2022-12-07 21:58:41.688547 Epoch 96, Training loss 0.6848797474218451\n",
            "2022-12-07 21:58:53.634609 Epoch 97, Training loss 0.6850318900307121\n",
            "2022-12-07 21:59:05.742020 Epoch 98, Training loss 0.6824079767974747\n",
            "2022-12-07 21:59:17.865297 Epoch 99, Training loss 0.680537128120737\n",
            "2022-12-07 21:59:29.813787 Epoch 100, Training loss 0.6790047590918553\n",
            "2022-12-07 21:59:41.829798 Epoch 101, Training loss 0.6780717439587464\n",
            "2022-12-07 21:59:53.942300 Epoch 102, Training loss 0.6768888400491241\n",
            "2022-12-07 22:00:05.936763 Epoch 103, Training loss 0.6738611489458157\n",
            "2022-12-07 22:00:18.052007 Epoch 104, Training loss 0.6719713930202567\n",
            "2022-12-07 22:00:30.046265 Epoch 105, Training loss 0.6700196912145371\n",
            "2022-12-07 22:00:42.081689 Epoch 106, Training loss 0.6700165469933044\n",
            "2022-12-07 22:00:54.079954 Epoch 107, Training loss 0.6671695906640319\n",
            "2022-12-07 22:01:06.237415 Epoch 108, Training loss 0.666107504149837\n",
            "2022-12-07 22:01:18.327349 Epoch 109, Training loss 0.6629919413181827\n",
            "2022-12-07 22:01:30.260566 Epoch 110, Training loss 0.6622791053236597\n",
            "2022-12-07 22:01:42.308425 Epoch 111, Training loss 0.6620044990268814\n",
            "2022-12-07 22:01:54.359840 Epoch 112, Training loss 0.6603573900278267\n",
            "2022-12-07 22:02:06.353403 Epoch 113, Training loss 0.6586150665341131\n",
            "2022-12-07 22:02:18.395727 Epoch 114, Training loss 0.6575363330219103\n",
            "2022-12-07 22:02:30.388054 Epoch 115, Training loss 0.6553481650703094\n",
            "2022-12-07 22:02:42.373820 Epoch 116, Training loss 0.653051585835569\n",
            "2022-12-07 22:02:54.553051 Epoch 117, Training loss 0.6527866182653496\n",
            "2022-12-07 22:03:06.652071 Epoch 118, Training loss 0.6514764340865947\n",
            "2022-12-07 22:03:18.736215 Epoch 119, Training loss 0.6503075576194411\n",
            "2022-12-07 22:03:30.743398 Epoch 120, Training loss 0.6479539706197845\n",
            "2022-12-07 22:03:42.758238 Epoch 121, Training loss 0.6469083599117406\n",
            "2022-12-07 22:03:54.674280 Epoch 122, Training loss 0.6454066284324812\n",
            "2022-12-07 22:04:06.698422 Epoch 123, Training loss 0.6425531025585312\n",
            "2022-12-07 22:04:18.773142 Epoch 124, Training loss 0.6418047961600296\n",
            "2022-12-07 22:04:30.831628 Epoch 125, Training loss 0.6431770974489124\n",
            "2022-12-07 22:04:42.861194 Epoch 126, Training loss 0.6401201107008073\n",
            "2022-12-07 22:04:54.764421 Epoch 127, Training loss 0.6388684226881207\n",
            "2022-12-07 22:05:06.666257 Epoch 128, Training loss 0.6376447293459607\n",
            "2022-12-07 22:05:18.660079 Epoch 129, Training loss 0.6375012512859481\n",
            "2022-12-07 22:05:30.593326 Epoch 130, Training loss 0.6345064955980272\n",
            "2022-12-07 22:05:42.511482 Epoch 131, Training loss 0.6344874159759267\n",
            "2022-12-07 22:05:54.457162 Epoch 132, Training loss 0.6317848539946939\n",
            "2022-12-07 22:06:06.417835 Epoch 133, Training loss 0.6308135803779373\n",
            "2022-12-07 22:06:18.543797 Epoch 134, Training loss 0.6293258533986938\n",
            "2022-12-07 22:06:30.597600 Epoch 135, Training loss 0.6275348914477527\n",
            "2022-12-07 22:06:42.604142 Epoch 136, Training loss 0.6274952794356115\n",
            "2022-12-07 22:06:54.582746 Epoch 137, Training loss 0.6247977561810437\n",
            "2022-12-07 22:07:06.503132 Epoch 138, Training loss 0.6245567651317857\n",
            "2022-12-07 22:07:18.460120 Epoch 139, Training loss 0.6234783344637708\n",
            "2022-12-07 22:07:30.477557 Epoch 140, Training loss 0.6227821557189498\n",
            "2022-12-07 22:07:42.407340 Epoch 141, Training loss 0.6202566316899132\n",
            "2022-12-07 22:07:54.426079 Epoch 142, Training loss 0.6179441996395131\n",
            "2022-12-07 22:08:06.460128 Epoch 143, Training loss 0.6175627004536216\n",
            "2022-12-07 22:08:18.434065 Epoch 144, Training loss 0.6184873819122534\n",
            "2022-12-07 22:08:30.418391 Epoch 145, Training loss 0.6164327110628338\n",
            "2022-12-07 22:08:42.350029 Epoch 146, Training loss 0.6155661278978333\n",
            "2022-12-07 22:08:54.308058 Epoch 147, Training loss 0.6143077500640889\n",
            "2022-12-07 22:09:06.291624 Epoch 148, Training loss 0.6135617641689223\n",
            "2022-12-07 22:09:18.307403 Epoch 149, Training loss 0.6123562841616628\n",
            "2022-12-07 22:09:30.389036 Epoch 150, Training loss 0.6106308176160773\n",
            "2022-12-07 22:09:42.420336 Epoch 151, Training loss 0.6100164458651067\n",
            "2022-12-07 22:09:54.349772 Epoch 152, Training loss 0.6084906038116006\n",
            "2022-12-07 22:10:06.340996 Epoch 153, Training loss 0.6070366502570375\n",
            "2022-12-07 22:10:18.366469 Epoch 154, Training loss 0.6092822678634883\n",
            "2022-12-07 22:10:30.356772 Epoch 155, Training loss 0.6078682941244081\n",
            "2022-12-07 22:10:42.315418 Epoch 156, Training loss 0.6052610912286412\n",
            "2022-12-07 22:10:54.253289 Epoch 157, Training loss 0.6044519667887627\n",
            "2022-12-07 22:11:06.324297 Epoch 158, Training loss 0.6024431956317419\n",
            "2022-12-07 22:11:18.324633 Epoch 159, Training loss 0.6022384713601578\n",
            "2022-12-07 22:11:30.368924 Epoch 160, Training loss 0.5993472939866888\n",
            "2022-12-07 22:11:42.441785 Epoch 161, Training loss 0.5997340428783461\n",
            "2022-12-07 22:11:54.477837 Epoch 162, Training loss 0.598091383876703\n",
            "2022-12-07 22:12:06.455742 Epoch 163, Training loss 0.5983839292660393\n",
            "2022-12-07 22:12:18.409617 Epoch 164, Training loss 0.5974148251592656\n",
            "2022-12-07 22:12:30.409165 Epoch 165, Training loss 0.5956996961703995\n",
            "2022-12-07 22:12:42.401096 Epoch 166, Training loss 0.5946451740725266\n",
            "2022-12-07 22:12:54.453337 Epoch 167, Training loss 0.5923898407946462\n",
            "2022-12-07 22:13:06.419553 Epoch 168, Training loss 0.5934633787178323\n",
            "2022-12-07 22:13:18.353463 Epoch 169, Training loss 0.5921615693727722\n",
            "2022-12-07 22:13:30.353447 Epoch 170, Training loss 0.5924377604899809\n",
            "2022-12-07 22:13:42.342433 Epoch 171, Training loss 0.591196673796\n",
            "2022-12-07 22:13:54.283145 Epoch 172, Training loss 0.5884349950591622\n",
            "2022-12-07 22:14:06.280334 Epoch 173, Training loss 0.5876545253235971\n",
            "2022-12-07 22:14:18.240206 Epoch 174, Training loss 0.588629739554337\n",
            "2022-12-07 22:14:30.278072 Epoch 175, Training loss 0.5873563614342828\n",
            "2022-12-07 22:14:42.342742 Epoch 176, Training loss 0.5858287964101947\n",
            "2022-12-07 22:14:54.336316 Epoch 177, Training loss 0.5847272062507431\n",
            "2022-12-07 22:15:06.445636 Epoch 178, Training loss 0.5844043446013995\n",
            "2022-12-07 22:15:18.452502 Epoch 179, Training loss 0.5824230226409405\n",
            "2022-12-07 22:15:30.424392 Epoch 180, Training loss 0.5819837369043809\n",
            "2022-12-07 22:15:42.503066 Epoch 181, Training loss 0.5817942326064305\n",
            "2022-12-07 22:15:54.439993 Epoch 182, Training loss 0.580891174504824\n",
            "2022-12-07 22:16:06.459683 Epoch 183, Training loss 0.5789419085244694\n",
            "2022-12-07 22:16:18.409783 Epoch 184, Training loss 0.5801515960708603\n",
            "2022-12-07 22:16:30.473807 Epoch 185, Training loss 0.5782681458517719\n",
            "2022-12-07 22:16:42.379570 Epoch 186, Training loss 0.5777706029012685\n",
            "2022-12-07 22:16:54.284520 Epoch 187, Training loss 0.5771776034551508\n",
            "2022-12-07 22:17:06.366524 Epoch 188, Training loss 0.5767646930025666\n",
            "2022-12-07 22:17:18.467755 Epoch 189, Training loss 0.5744584600448304\n",
            "2022-12-07 22:17:30.474557 Epoch 190, Training loss 0.5744190998089588\n",
            "2022-12-07 22:17:42.473530 Epoch 191, Training loss 0.573390755164044\n",
            "2022-12-07 22:17:54.436192 Epoch 192, Training loss 0.5729396715950783\n",
            "2022-12-07 22:18:06.520623 Epoch 193, Training loss 0.5709291335261996\n",
            "2022-12-07 22:18:18.737096 Epoch 194, Training loss 0.5728331882973461\n",
            "2022-12-07 22:18:30.933082 Epoch 195, Training loss 0.5702312791057865\n",
            "2022-12-07 22:18:43.042349 Epoch 196, Training loss 0.5693387638424974\n",
            "2022-12-07 22:18:55.165960 Epoch 197, Training loss 0.5686743617286463\n",
            "2022-12-07 22:19:07.305415 Epoch 198, Training loss 0.5661740142404271\n",
            "2022-12-07 22:19:19.402174 Epoch 199, Training loss 0.5673700945685282\n",
            "2022-12-07 22:19:31.504202 Epoch 200, Training loss 0.5671954188886505\n",
            "2022-12-07 22:19:43.544500 Epoch 201, Training loss 0.5667818328151313\n",
            "2022-12-07 22:19:55.598745 Epoch 202, Training loss 0.5668314730419832\n",
            "2022-12-07 22:20:07.584212 Epoch 203, Training loss 0.5646646404281601\n",
            "2022-12-07 22:20:19.699181 Epoch 204, Training loss 0.5655662306510579\n",
            "2022-12-07 22:20:31.854632 Epoch 205, Training loss 0.5628771820412878\n",
            "2022-12-07 22:20:43.959682 Epoch 206, Training loss 0.564192281583386\n",
            "2022-12-07 22:20:56.119247 Epoch 207, Training loss 0.5612920332328438\n",
            "2022-12-07 22:21:08.096635 Epoch 208, Training loss 0.5614366584726612\n",
            "2022-12-07 22:21:20.166963 Epoch 209, Training loss 0.5603648168237313\n",
            "2022-12-07 22:21:32.220540 Epoch 210, Training loss 0.5611333433167099\n",
            "2022-12-07 22:21:44.329694 Epoch 211, Training loss 0.5609584709491267\n",
            "2022-12-07 22:21:56.342172 Epoch 212, Training loss 0.5580049344645742\n",
            "2022-12-07 22:22:08.405331 Epoch 213, Training loss 0.5582520640872018\n",
            "2022-12-07 22:22:20.443249 Epoch 214, Training loss 0.5559637509572232\n",
            "2022-12-07 22:22:32.384989 Epoch 215, Training loss 0.5561602533320942\n",
            "2022-12-07 22:22:44.401354 Epoch 216, Training loss 0.556171974562623\n",
            "2022-12-07 22:22:56.407530 Epoch 217, Training loss 0.5545305798349478\n",
            "2022-12-07 22:23:08.589280 Epoch 218, Training loss 0.5542502895645474\n",
            "2022-12-07 22:23:20.687622 Epoch 219, Training loss 0.5528354413445343\n",
            "2022-12-07 22:23:32.824989 Epoch 220, Training loss 0.5541794863731965\n",
            "2022-12-07 22:23:44.901434 Epoch 221, Training loss 0.5533706747814823\n",
            "2022-12-07 22:23:56.924492 Epoch 222, Training loss 0.5522603310282578\n",
            "2022-12-07 22:24:09.014608 Epoch 223, Training loss 0.5512022323087048\n",
            "2022-12-07 22:24:20.959888 Epoch 224, Training loss 0.5517476840549723\n",
            "2022-12-07 22:24:33.063307 Epoch 225, Training loss 0.5487333757378866\n",
            "2022-12-07 22:24:45.148457 Epoch 226, Training loss 0.5512822765066191\n",
            "2022-12-07 22:24:57.201571 Epoch 227, Training loss 0.5494207162076555\n",
            "2022-12-07 22:25:09.286681 Epoch 228, Training loss 0.5478407646460302\n",
            "2022-12-07 22:25:21.437513 Epoch 229, Training loss 0.5463051366264863\n",
            "2022-12-07 22:25:33.576974 Epoch 230, Training loss 0.547839756207088\n",
            "2022-12-07 22:25:45.512262 Epoch 231, Training loss 0.5459391228149614\n",
            "2022-12-07 22:25:57.497143 Epoch 232, Training loss 0.5448002130212382\n",
            "2022-12-07 22:26:09.615688 Epoch 233, Training loss 0.5460041644018324\n",
            "2022-12-07 22:26:21.681237 Epoch 234, Training loss 0.543800538077074\n",
            "2022-12-07 22:26:33.709342 Epoch 235, Training loss 0.5450829939387948\n",
            "2022-12-07 22:26:45.738396 Epoch 236, Training loss 0.54319211981638\n",
            "2022-12-07 22:26:57.805429 Epoch 237, Training loss 0.5430281201134557\n",
            "2022-12-07 22:27:09.781685 Epoch 238, Training loss 0.5430881627799605\n",
            "2022-12-07 22:27:21.776861 Epoch 239, Training loss 0.5412198994928004\n",
            "2022-12-07 22:27:33.732160 Epoch 240, Training loss 0.5404421487808837\n",
            "2022-12-07 22:27:45.716345 Epoch 241, Training loss 0.5409996907043335\n",
            "2022-12-07 22:27:57.650082 Epoch 242, Training loss 0.5406403679143438\n",
            "2022-12-07 22:28:09.499050 Epoch 243, Training loss 0.5399496874693409\n",
            "2022-12-07 22:28:21.762573 Epoch 244, Training loss 0.5384411512280974\n",
            "2022-12-07 22:28:33.878407 Epoch 245, Training loss 0.5365483125914698\n",
            "2022-12-07 22:28:45.982344 Epoch 246, Training loss 0.5401380439014996\n",
            "2022-12-07 22:28:58.004625 Epoch 247, Training loss 0.5364597232445426\n",
            "2022-12-07 22:29:10.102337 Epoch 248, Training loss 0.5362879456499653\n",
            "2022-12-07 22:29:22.288826 Epoch 249, Training loss 0.536370146305055\n",
            "2022-12-07 22:29:34.351978 Epoch 250, Training loss 0.5353945822971861\n",
            "2022-12-07 22:29:46.361329 Epoch 251, Training loss 0.5339251379375263\n",
            "2022-12-07 22:29:58.474157 Epoch 252, Training loss 0.5341257834449753\n",
            "2022-12-07 22:30:10.469193 Epoch 253, Training loss 0.5341130486306023\n",
            "2022-12-07 22:30:22.531595 Epoch 254, Training loss 0.5341403202708724\n",
            "2022-12-07 22:30:34.732908 Epoch 255, Training loss 0.5330484962028921\n",
            "2022-12-07 22:30:46.782224 Epoch 256, Training loss 0.5312761847701524\n",
            "2022-12-07 22:30:58.779715 Epoch 257, Training loss 0.5323889233800762\n",
            "2022-12-07 22:31:10.754800 Epoch 258, Training loss 0.5310651118798024\n",
            "2022-12-07 22:31:22.850665 Epoch 259, Training loss 0.5303491278720633\n",
            "2022-12-07 22:31:34.804969 Epoch 260, Training loss 0.5284294975954859\n",
            "2022-12-07 22:31:46.897484 Epoch 261, Training loss 0.5296142506972908\n",
            "2022-12-07 22:31:58.883285 Epoch 262, Training loss 0.5288355058759374\n",
            "2022-12-07 22:32:10.947402 Epoch 263, Training loss 0.5291179179040062\n",
            "2022-12-07 22:32:22.914813 Epoch 264, Training loss 0.5282324408859853\n",
            "2022-12-07 22:32:35.015622 Epoch 265, Training loss 0.5281099597435168\n",
            "2022-12-07 22:32:47.177871 Epoch 266, Training loss 0.5268927003111681\n",
            "2022-12-07 22:32:59.344391 Epoch 267, Training loss 0.5275408367404852\n",
            "2022-12-07 22:33:11.552339 Epoch 268, Training loss 0.5275611033677445\n",
            "2022-12-07 22:33:23.740489 Epoch 269, Training loss 0.5257406816686816\n",
            "2022-12-07 22:33:35.861814 Epoch 270, Training loss 0.5241553557993811\n",
            "2022-12-07 22:33:47.944682 Epoch 271, Training loss 0.5254475240931487\n",
            "2022-12-07 22:34:00.065890 Epoch 272, Training loss 0.52375673735157\n",
            "2022-12-07 22:34:12.219332 Epoch 273, Training loss 0.5228968689889859\n",
            "2022-12-07 22:34:24.407624 Epoch 274, Training loss 0.5224807562539949\n",
            "2022-12-07 22:34:36.550788 Epoch 275, Training loss 0.5228931302457209\n",
            "2022-12-07 22:34:48.592434 Epoch 276, Training loss 0.522371592728988\n",
            "2022-12-07 22:35:00.588375 Epoch 277, Training loss 0.522496636108974\n",
            "2022-12-07 22:35:12.812929 Epoch 278, Training loss 0.5210294907012254\n",
            "2022-12-07 22:35:24.881874 Epoch 279, Training loss 0.522321328825658\n",
            "2022-12-07 22:35:37.035270 Epoch 280, Training loss 0.5204090865524224\n",
            "2022-12-07 22:35:49.066019 Epoch 281, Training loss 0.5203527702051965\n",
            "2022-12-07 22:36:01.308343 Epoch 282, Training loss 0.520926663828323\n",
            "2022-12-07 22:36:13.400794 Epoch 283, Training loss 0.5194029062414718\n",
            "2022-12-07 22:36:25.492096 Epoch 284, Training loss 0.518188300340072\n",
            "2022-12-07 22:36:37.567764 Epoch 285, Training loss 0.5189679058082878\n",
            "2022-12-07 22:36:49.757360 Epoch 286, Training loss 0.5186965788721734\n",
            "2022-12-07 22:37:01.832000 Epoch 287, Training loss 0.5175895065145419\n",
            "2022-12-07 22:37:13.910475 Epoch 288, Training loss 0.5173784657512479\n",
            "2022-12-07 22:37:25.949917 Epoch 289, Training loss 0.5163889707392438\n",
            "2022-12-07 22:37:38.155017 Epoch 290, Training loss 0.5166762447951699\n",
            "2022-12-07 22:37:50.294102 Epoch 291, Training loss 0.5150375015785932\n",
            "2022-12-07 22:38:02.517090 Epoch 292, Training loss 0.5173024667612732\n",
            "2022-12-07 22:38:14.625856 Epoch 293, Training loss 0.5157493106506364\n",
            "2022-12-07 22:38:26.717320 Epoch 294, Training loss 0.5148244353816332\n",
            "2022-12-07 22:38:38.818314 Epoch 295, Training loss 0.5136221115241575\n",
            "2022-12-07 22:38:50.880576 Epoch 296, Training loss 0.5140091536752404\n",
            "2022-12-07 22:39:02.895867 Epoch 297, Training loss 0.5145305592919249\n",
            "2022-12-07 22:39:14.964118 Epoch 298, Training loss 0.5152726788121416\n",
            "2022-12-07 22:39:27.098773 Epoch 299, Training loss 0.5132027913237471\n",
            "2022-12-07 22:39:39.176983 Epoch 300, Training loss 0.5128388130832511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                          shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                        shuffle=False)\n",
        "validate(model, train_loader, val_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vx9g2-E4xShL",
        "outputId": "b2de3f97-b7fa-4673-aa71-e122434f51f9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.82\n",
            "Accuracy val: 0.62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part B, weight decay"
      ],
      "metadata": {
        "id": "yUHPl9JQxahm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(n_epochs, optimizer, model, loss_fn, \n",
        "                  train_loader):\n",
        "  for epoch in range(1, n_epochs +1):\n",
        "    loss_train = 0.0\n",
        "    for imgs, labels in train_loader:\n",
        "      outputs = model(imgs.to('cuda:0'))\n",
        "      loss = loss_fn(outputs.to('cuda:0'), \n",
        "                     labels.to('cuda:0'))\n",
        "      \n",
        "      ambda = 0.001\n",
        "      norm = sum(p.pow(2.0).sum()\n",
        "                    for p in model.parameters())\n",
        "    \n",
        "      loss = loss + ambda*norm\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      loss_train += loss.item()\n",
        "     \n",
        "    print('{} Epoch {}, Training Loss {}'.format(datetime.datetime.now(),\n",
        "                                    epoch, loss_train / len(train_loader)))\n"
      ],
      "metadata": {
        "id": "0vIhW4LixWc8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, \n",
        "                    batch_size=64, shuffle=True)\n",
        "\n",
        "model = Net().to('cuda:0')\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs =300,\n",
        "    optimizer= optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        "    \n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_ggOhdQxod0",
        "outputId": "cf6c26e0-fc59-4188-e437-6b0d877a35f3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-12-07 22:58:12.849836 Epoch 1, Training Loss 2.0841912031173706\n",
            "2022-12-07 22:58:26.036678 Epoch 2, Training Loss 1.8432978106581646\n",
            "2022-12-07 22:58:39.258628 Epoch 3, Training Loss 1.6726413815832504\n",
            "2022-12-07 22:58:52.378705 Epoch 4, Training Loss 1.5799622424423236\n",
            "2022-12-07 22:59:05.474073 Epoch 5, Training Loss 1.5235693555353853\n",
            "2022-12-07 22:59:18.584566 Epoch 6, Training Loss 1.4768045788530804\n",
            "2022-12-07 22:59:31.614084 Epoch 7, Training Loss 1.432058575817996\n",
            "2022-12-07 22:59:44.675516 Epoch 8, Training Loss 1.3874695857467554\n",
            "2022-12-07 22:59:57.645479 Epoch 9, Training Loss 1.3414176331304224\n",
            "2022-12-07 23:00:10.691019 Epoch 10, Training Loss 1.3009408210092188\n",
            "2022-12-07 23:00:23.744659 Epoch 11, Training Loss 1.2690469714076928\n",
            "2022-12-07 23:00:36.918254 Epoch 12, Training Loss 1.2431152326524104\n",
            "2022-12-07 23:00:49.998278 Epoch 13, Training Loss 1.2232147888149447\n",
            "2022-12-07 23:01:02.865343 Epoch 14, Training Loss 1.2052133106209737\n",
            "2022-12-07 23:01:15.891741 Epoch 15, Training Loss 1.1876779014192274\n",
            "2022-12-07 23:01:28.985857 Epoch 16, Training Loss 1.1735216322762276\n",
            "2022-12-07 23:01:42.115224 Epoch 17, Training Loss 1.1566410325372312\n",
            "2022-12-07 23:01:55.337395 Epoch 18, Training Loss 1.1437616070822987\n",
            "2022-12-07 23:02:08.521109 Epoch 19, Training Loss 1.1325259796341363\n",
            "2022-12-07 23:02:21.619451 Epoch 20, Training Loss 1.1198059232033732\n",
            "2022-12-07 23:02:34.733725 Epoch 21, Training Loss 1.1096581407367725\n",
            "2022-12-07 23:02:48.034360 Epoch 22, Training Loss 1.099997643314664\n",
            "2022-12-07 23:03:01.269667 Epoch 23, Training Loss 1.0915731324259277\n",
            "2022-12-07 23:03:14.410260 Epoch 24, Training Loss 1.0843223451500963\n",
            "2022-12-07 23:03:27.514836 Epoch 25, Training Loss 1.0773473552730688\n",
            "2022-12-07 23:03:40.585974 Epoch 26, Training Loss 1.0737754563084039\n",
            "2022-12-07 23:03:53.606672 Epoch 27, Training Loss 1.065823925578076\n",
            "2022-12-07 23:04:06.560007 Epoch 28, Training Loss 1.0618125641589884\n",
            "2022-12-07 23:04:19.619893 Epoch 29, Training Loss 1.054614696889887\n",
            "2022-12-07 23:04:32.566275 Epoch 30, Training Loss 1.0527035850088309\n",
            "2022-12-07 23:04:45.520787 Epoch 31, Training Loss 1.0462716150924067\n",
            "2022-12-07 23:04:58.580639 Epoch 32, Training Loss 1.040261395294648\n",
            "2022-12-07 23:05:11.711846 Epoch 33, Training Loss 1.0377191415680644\n",
            "2022-12-07 23:05:24.927632 Epoch 34, Training Loss 1.033922750855346\n",
            "2022-12-07 23:05:38.122070 Epoch 35, Training Loss 1.0307574795030268\n",
            "2022-12-07 23:05:51.385670 Epoch 36, Training Loss 1.0251369915350015\n",
            "2022-12-07 23:06:04.468037 Epoch 37, Training Loss 1.0233935743494107\n",
            "2022-12-07 23:06:17.661309 Epoch 38, Training Loss 1.0196219552355958\n",
            "2022-12-07 23:06:30.753766 Epoch 39, Training Loss 1.0153910544369837\n",
            "2022-12-07 23:06:43.761758 Epoch 40, Training Loss 1.0126676595272006\n",
            "2022-12-07 23:06:56.706532 Epoch 41, Training Loss 1.008549907552007\n",
            "2022-12-07 23:07:09.856156 Epoch 42, Training Loss 1.0059093180519845\n",
            "2022-12-07 23:07:22.916777 Epoch 43, Training Loss 1.003170672051437\n",
            "2022-12-07 23:07:35.911320 Epoch 44, Training Loss 1.0001540873056787\n",
            "2022-12-07 23:07:48.911431 Epoch 45, Training Loss 0.997520772926033\n",
            "2022-12-07 23:08:02.046156 Epoch 46, Training Loss 0.9940744716950389\n",
            "2022-12-07 23:08:15.146304 Epoch 47, Training Loss 0.9915044385453929\n",
            "2022-12-07 23:08:28.254847 Epoch 48, Training Loss 0.989563134930018\n",
            "2022-12-07 23:08:41.097245 Epoch 49, Training Loss 0.988123220403481\n",
            "2022-12-07 23:08:54.106453 Epoch 50, Training Loss 0.9860581935519148\n",
            "2022-12-07 23:09:07.166025 Epoch 51, Training Loss 0.9829963454047738\n",
            "2022-12-07 23:09:20.216207 Epoch 52, Training Loss 0.9796517039351451\n",
            "2022-12-07 23:09:33.253814 Epoch 53, Training Loss 0.9779117768225463\n",
            "2022-12-07 23:09:46.222422 Epoch 54, Training Loss 0.975545890877009\n",
            "2022-12-07 23:09:59.250246 Epoch 55, Training Loss 0.9737013971714108\n",
            "2022-12-07 23:10:12.301133 Epoch 56, Training Loss 0.9742761497454875\n",
            "2022-12-07 23:10:25.360385 Epoch 57, Training Loss 0.9701925830920334\n",
            "2022-12-07 23:10:38.308655 Epoch 58, Training Loss 0.969899722224916\n",
            "2022-12-07 23:10:51.300504 Epoch 59, Training Loss 0.9682228299205565\n",
            "2022-12-07 23:11:04.361485 Epoch 60, Training Loss 0.9654403213802201\n",
            "2022-12-07 23:11:17.415193 Epoch 61, Training Loss 0.9641662291858507\n",
            "2022-12-07 23:11:30.497887 Epoch 62, Training Loss 0.9628031518300781\n",
            "2022-12-07 23:11:43.570161 Epoch 63, Training Loss 0.9617103217050548\n",
            "2022-12-07 23:11:56.567522 Epoch 64, Training Loss 0.9605453072301567\n",
            "2022-12-07 23:12:09.597570 Epoch 65, Training Loss 0.9584945523373002\n",
            "2022-12-07 23:12:22.851082 Epoch 66, Training Loss 0.9573733514684546\n",
            "2022-12-07 23:12:36.026491 Epoch 67, Training Loss 0.9553572528654962\n",
            "2022-12-07 23:12:49.074199 Epoch 68, Training Loss 0.9545121014575519\n",
            "2022-12-07 23:13:02.128299 Epoch 69, Training Loss 0.9531158667696101\n",
            "2022-12-07 23:13:15.226551 Epoch 70, Training Loss 0.9523707879016466\n",
            "2022-12-07 23:13:28.283191 Epoch 71, Training Loss 0.951576323155552\n",
            "2022-12-07 23:13:41.362660 Epoch 72, Training Loss 0.9478911668290873\n",
            "2022-12-07 23:13:54.392079 Epoch 73, Training Loss 0.9484698600171472\n",
            "2022-12-07 23:14:07.529801 Epoch 74, Training Loss 0.9477556745719422\n",
            "2022-12-07 23:14:20.493714 Epoch 75, Training Loss 0.9475359979188046\n",
            "2022-12-07 23:14:33.505347 Epoch 76, Training Loss 0.9464355059292006\n",
            "2022-12-07 23:14:46.475987 Epoch 77, Training Loss 0.9456897016681368\n",
            "2022-12-07 23:14:59.627169 Epoch 78, Training Loss 0.9453783558915033\n",
            "2022-12-07 23:15:12.751283 Epoch 79, Training Loss 0.941811587728198\n",
            "2022-12-07 23:15:25.838945 Epoch 80, Training Loss 0.9425048559827878\n",
            "2022-12-07 23:15:38.944950 Epoch 81, Training Loss 0.9410297225808244\n",
            "2022-12-07 23:15:52.135644 Epoch 82, Training Loss 0.939419819647089\n",
            "2022-12-07 23:16:05.256423 Epoch 83, Training Loss 0.9406249518589596\n",
            "2022-12-07 23:16:18.477577 Epoch 84, Training Loss 0.9393151102925811\n",
            "2022-12-07 23:16:31.795079 Epoch 85, Training Loss 0.9366653999099341\n",
            "2022-12-07 23:16:45.104200 Epoch 86, Training Loss 0.937290015275521\n",
            "2022-12-07 23:16:58.448985 Epoch 87, Training Loss 0.9372533905841506\n",
            "2022-12-07 23:17:11.744039 Epoch 88, Training Loss 0.9366700567705247\n",
            "2022-12-07 23:17:24.870245 Epoch 89, Training Loss 0.9357633254564631\n",
            "2022-12-07 23:17:37.819840 Epoch 90, Training Loss 0.9344858391510557\n",
            "2022-12-07 23:17:50.870610 Epoch 91, Training Loss 0.9338982864414029\n",
            "2022-12-07 23:18:04.049569 Epoch 92, Training Loss 0.9335496577308001\n",
            "2022-12-07 23:18:17.097316 Epoch 93, Training Loss 0.9342159886494317\n",
            "2022-12-07 23:18:30.149861 Epoch 94, Training Loss 0.9317909728383165\n",
            "2022-12-07 23:18:43.233158 Epoch 95, Training Loss 0.932352135324722\n",
            "2022-12-07 23:18:56.235692 Epoch 96, Training Loss 0.931139193532412\n",
            "2022-12-07 23:19:09.272703 Epoch 97, Training Loss 0.9296551906239346\n",
            "2022-12-07 23:19:22.336220 Epoch 98, Training Loss 0.9298754724700128\n",
            "2022-12-07 23:19:35.430709 Epoch 99, Training Loss 0.9296207079649581\n",
            "2022-12-07 23:19:48.617337 Epoch 100, Training Loss 0.9273848721895681\n",
            "2022-12-07 23:20:01.803398 Epoch 101, Training Loss 0.9276161483486595\n",
            "2022-12-07 23:20:14.944546 Epoch 102, Training Loss 0.9275052719713782\n",
            "2022-12-07 23:20:28.036225 Epoch 103, Training Loss 0.9265055333257026\n",
            "2022-12-07 23:20:41.167231 Epoch 104, Training Loss 0.9253576027462854\n",
            "2022-12-07 23:20:54.265952 Epoch 105, Training Loss 0.9263354532249138\n",
            "2022-12-07 23:21:07.456425 Epoch 106, Training Loss 0.9252716722085957\n",
            "2022-12-07 23:21:20.563700 Epoch 107, Training Loss 0.9247388126295241\n",
            "2022-12-07 23:21:33.637193 Epoch 108, Training Loss 0.9243232142132567\n",
            "2022-12-07 23:21:46.858282 Epoch 109, Training Loss 0.9247753447888757\n",
            "2022-12-07 23:21:59.827232 Epoch 110, Training Loss 0.9232308530746518\n",
            "2022-12-07 23:22:12.888197 Epoch 111, Training Loss 0.9218909098669086\n",
            "2022-12-07 23:22:26.042363 Epoch 112, Training Loss 0.9241836590840079\n",
            "2022-12-07 23:22:39.067434 Epoch 113, Training Loss 0.9221077145213057\n",
            "2022-12-07 23:22:52.201215 Epoch 114, Training Loss 0.9215449416424002\n",
            "2022-12-07 23:23:05.463100 Epoch 115, Training Loss 0.9202255589120528\n",
            "2022-12-07 23:23:18.778507 Epoch 116, Training Loss 0.9215315253380925\n",
            "2022-12-07 23:23:32.197660 Epoch 117, Training Loss 0.9210511846920414\n",
            "2022-12-07 23:23:45.404520 Epoch 118, Training Loss 0.9211856399655647\n",
            "2022-12-07 23:23:58.728824 Epoch 119, Training Loss 0.9199240575818455\n",
            "2022-12-07 23:24:11.997761 Epoch 120, Training Loss 0.9198059922898821\n",
            "2022-12-07 23:24:25.250158 Epoch 121, Training Loss 0.9191807536670314\n",
            "2022-12-07 23:24:38.475691 Epoch 122, Training Loss 0.9195276671053504\n",
            "2022-12-07 23:24:51.785489 Epoch 123, Training Loss 0.919193455020485\n",
            "2022-12-07 23:25:04.981073 Epoch 124, Training Loss 0.9174380250599073\n",
            "2022-12-07 23:25:18.142204 Epoch 125, Training Loss 0.916793658605317\n",
            "2022-12-07 23:25:31.269329 Epoch 126, Training Loss 0.9175088697534692\n",
            "2022-12-07 23:25:44.298059 Epoch 127, Training Loss 0.9159202440772825\n",
            "2022-12-07 23:25:57.389801 Epoch 128, Training Loss 0.9166160594776768\n",
            "2022-12-07 23:26:10.489200 Epoch 129, Training Loss 0.9170922926624717\n",
            "2022-12-07 23:26:23.603902 Epoch 130, Training Loss 0.9163952700774688\n",
            "2022-12-07 23:26:36.685279 Epoch 131, Training Loss 0.9149838730197428\n",
            "2022-12-07 23:26:49.730111 Epoch 132, Training Loss 0.9162014131350895\n",
            "2022-12-07 23:27:02.723910 Epoch 133, Training Loss 0.9150697784807981\n",
            "2022-12-07 23:27:15.643912 Epoch 134, Training Loss 0.9152038122534447\n",
            "2022-12-07 23:27:28.498620 Epoch 135, Training Loss 0.9135859815970712\n",
            "2022-12-07 23:27:41.509159 Epoch 136, Training Loss 0.9136856120565663\n",
            "2022-12-07 23:27:54.644205 Epoch 137, Training Loss 0.9148926887365864\n",
            "2022-12-07 23:28:07.717725 Epoch 138, Training Loss 0.9143108814726095\n",
            "2022-12-07 23:28:20.663464 Epoch 139, Training Loss 0.912814078504777\n",
            "2022-12-07 23:28:33.700423 Epoch 140, Training Loss 0.9123891297813571\n",
            "2022-12-07 23:28:46.789138 Epoch 141, Training Loss 0.9133048322804443\n",
            "2022-12-07 23:28:59.803342 Epoch 142, Training Loss 0.912293829805101\n",
            "2022-12-07 23:29:12.735816 Epoch 143, Training Loss 0.9126866352375206\n",
            "2022-12-07 23:29:25.684242 Epoch 144, Training Loss 0.9112917710753048\n",
            "2022-12-07 23:29:38.651724 Epoch 145, Training Loss 0.9133170791294264\n",
            "2022-12-07 23:29:51.729768 Epoch 146, Training Loss 0.9117731749249236\n",
            "2022-12-07 23:30:04.776814 Epoch 147, Training Loss 0.9117043442128564\n",
            "2022-12-07 23:30:17.786769 Epoch 148, Training Loss 0.9118452899901154\n",
            "2022-12-07 23:30:30.841317 Epoch 149, Training Loss 0.9119542402684536\n",
            "2022-12-07 23:30:43.857374 Epoch 150, Training Loss 0.9111580476736474\n",
            "2022-12-07 23:30:56.954735 Epoch 151, Training Loss 0.9121445620151432\n",
            "2022-12-07 23:31:10.011180 Epoch 152, Training Loss 0.9112980499139527\n",
            "2022-12-07 23:31:23.037743 Epoch 153, Training Loss 0.9091098745308264\n",
            "2022-12-07 23:31:36.055578 Epoch 154, Training Loss 0.9094563630383338\n",
            "2022-12-07 23:31:49.133181 Epoch 155, Training Loss 0.9099535214169251\n",
            "2022-12-07 23:32:02.265965 Epoch 156, Training Loss 0.9098250741696419\n",
            "2022-12-07 23:32:15.309453 Epoch 157, Training Loss 0.9078750281840029\n",
            "2022-12-07 23:32:28.322646 Epoch 158, Training Loss 0.9086631326876637\n",
            "2022-12-07 23:32:41.300729 Epoch 159, Training Loss 0.9079886744241885\n",
            "2022-12-07 23:32:54.232953 Epoch 160, Training Loss 0.9087185248389573\n",
            "2022-12-07 23:33:07.297955 Epoch 161, Training Loss 0.9074562012844378\n",
            "2022-12-07 23:33:20.368392 Epoch 162, Training Loss 0.909101607854409\n",
            "2022-12-07 23:33:33.424259 Epoch 163, Training Loss 0.908821704861758\n",
            "2022-12-07 23:33:46.516676 Epoch 164, Training Loss 0.9080124787052574\n",
            "2022-12-07 23:33:59.512684 Epoch 165, Training Loss 0.9091547187941763\n",
            "2022-12-07 23:34:12.668576 Epoch 166, Training Loss 0.9084466133276214\n",
            "2022-12-07 23:34:25.694549 Epoch 167, Training Loss 0.90699261952849\n",
            "2022-12-07 23:34:38.787459 Epoch 168, Training Loss 0.9073983978889787\n",
            "2022-12-07 23:34:51.770192 Epoch 169, Training Loss 0.9073426092372221\n",
            "2022-12-07 23:35:04.758204 Epoch 170, Training Loss 0.9069657581846428\n",
            "2022-12-07 23:35:17.786850 Epoch 171, Training Loss 0.9063614809604557\n",
            "2022-12-07 23:35:30.762359 Epoch 172, Training Loss 0.9060084597229043\n",
            "2022-12-07 23:35:43.944818 Epoch 173, Training Loss 0.9085058617165022\n",
            "2022-12-07 23:35:57.091638 Epoch 174, Training Loss 0.9075871018497536\n",
            "2022-12-07 23:36:10.329497 Epoch 175, Training Loss 0.9057112678008921\n",
            "2022-12-07 23:36:23.595311 Epoch 176, Training Loss 0.9087191033546272\n",
            "2022-12-07 23:36:36.617485 Epoch 177, Training Loss 0.9058609694013815\n",
            "2022-12-07 23:36:49.678991 Epoch 178, Training Loss 0.9057818784585694\n",
            "2022-12-07 23:37:02.826464 Epoch 179, Training Loss 0.9045554045825968\n",
            "2022-12-07 23:37:16.028537 Epoch 180, Training Loss 0.9067797900923072\n",
            "2022-12-07 23:37:29.279732 Epoch 181, Training Loss 0.9053703304141989\n",
            "2022-12-07 23:37:42.466443 Epoch 182, Training Loss 0.904286182978574\n",
            "2022-12-07 23:37:55.641174 Epoch 183, Training Loss 0.9041736399578622\n",
            "2022-12-07 23:38:08.772691 Epoch 184, Training Loss 0.9036900978868879\n",
            "2022-12-07 23:38:21.775597 Epoch 185, Training Loss 0.9047261100748311\n",
            "2022-12-07 23:38:34.805246 Epoch 186, Training Loss 0.905223868615792\n",
            "2022-12-07 23:38:48.121455 Epoch 187, Training Loss 0.9036517997684381\n",
            "2022-12-07 23:39:01.239091 Epoch 188, Training Loss 0.9038692011552698\n",
            "2022-12-07 23:39:14.277340 Epoch 189, Training Loss 0.9021547893276605\n",
            "2022-12-07 23:39:27.366010 Epoch 190, Training Loss 0.9045460539705613\n",
            "2022-12-07 23:39:40.523903 Epoch 191, Training Loss 0.9025324787325262\n",
            "2022-12-07 23:39:53.592310 Epoch 192, Training Loss 0.9032032524075959\n",
            "2022-12-07 23:40:06.693755 Epoch 193, Training Loss 0.9017183620606541\n",
            "2022-12-07 23:40:19.832827 Epoch 194, Training Loss 0.9030134111566617\n",
            "2022-12-07 23:40:33.036989 Epoch 195, Training Loss 0.9038851187204766\n",
            "2022-12-07 23:40:46.296228 Epoch 196, Training Loss 0.9047114279721399\n",
            "2022-12-07 23:40:59.478322 Epoch 197, Training Loss 0.9029575062682257\n",
            "2022-12-07 23:41:12.741501 Epoch 198, Training Loss 0.9032679414352798\n",
            "2022-12-07 23:41:25.949392 Epoch 199, Training Loss 0.902140064617557\n",
            "2022-12-07 23:41:39.144621 Epoch 200, Training Loss 0.9039525736475844\n",
            "2022-12-07 23:41:52.348260 Epoch 201, Training Loss 0.9020410796717915\n",
            "2022-12-07 23:42:05.483955 Epoch 202, Training Loss 0.9025265839703552\n",
            "2022-12-07 23:42:18.620510 Epoch 203, Training Loss 0.902796268082031\n",
            "2022-12-07 23:42:31.749594 Epoch 204, Training Loss 0.9010112705590475\n",
            "2022-12-07 23:42:44.931899 Epoch 205, Training Loss 0.9029480909447536\n",
            "2022-12-07 23:42:58.116464 Epoch 206, Training Loss 0.9018747412487674\n",
            "2022-12-07 23:43:11.301155 Epoch 207, Training Loss 0.9020431928927332\n",
            "2022-12-07 23:43:24.531702 Epoch 208, Training Loss 0.9008203115304718\n",
            "2022-12-07 23:43:37.783843 Epoch 209, Training Loss 0.900621453971814\n",
            "2022-12-07 23:43:51.108742 Epoch 210, Training Loss 0.9015062943748806\n",
            "2022-12-07 23:44:04.433491 Epoch 211, Training Loss 0.899852105571181\n",
            "2022-12-07 23:44:17.645073 Epoch 212, Training Loss 0.9013335903739685\n",
            "2022-12-07 23:44:30.805472 Epoch 213, Training Loss 0.8999996413202846\n",
            "2022-12-07 23:44:44.054443 Epoch 214, Training Loss 0.9003923340984012\n",
            "2022-12-07 23:44:57.269546 Epoch 215, Training Loss 0.9006029365922484\n",
            "2022-12-07 23:45:10.463698 Epoch 216, Training Loss 0.8998364729954459\n",
            "2022-12-07 23:45:23.588142 Epoch 217, Training Loss 0.8995738315307881\n",
            "2022-12-07 23:45:36.732747 Epoch 218, Training Loss 0.8997356164485902\n",
            "2022-12-07 23:45:49.947398 Epoch 219, Training Loss 0.899775336236905\n",
            "2022-12-07 23:46:03.160875 Epoch 220, Training Loss 0.8998034164271391\n",
            "2022-12-07 23:46:16.544235 Epoch 221, Training Loss 0.9003451774492288\n",
            "2022-12-07 23:46:29.757288 Epoch 222, Training Loss 0.8995239720167711\n",
            "2022-12-07 23:46:42.961471 Epoch 223, Training Loss 0.8990983014064067\n",
            "2022-12-07 23:46:56.115407 Epoch 224, Training Loss 0.8983742832527746\n",
            "2022-12-07 23:47:09.268131 Epoch 225, Training Loss 0.8997017658122665\n",
            "2022-12-07 23:47:22.435497 Epoch 226, Training Loss 0.899495770711728\n",
            "2022-12-07 23:47:35.586516 Epoch 227, Training Loss 0.8998667401883303\n",
            "2022-12-07 23:47:48.832236 Epoch 228, Training Loss 0.8994031131572431\n",
            "2022-12-07 23:48:01.979461 Epoch 229, Training Loss 0.8996894606543929\n",
            "2022-12-07 23:48:15.222225 Epoch 230, Training Loss 0.8983829292037603\n",
            "2022-12-07 23:48:28.309627 Epoch 231, Training Loss 0.8975888153018854\n",
            "2022-12-07 23:48:41.438029 Epoch 232, Training Loss 0.8992823396649812\n",
            "2022-12-07 23:48:54.541072 Epoch 233, Training Loss 0.898261691374547\n",
            "2022-12-07 23:49:07.816737 Epoch 234, Training Loss 0.8988104688237085\n",
            "2022-12-07 23:49:21.019537 Epoch 235, Training Loss 0.8987020516334592\n",
            "2022-12-07 23:49:34.363286 Epoch 236, Training Loss 0.8991959519550928\n",
            "2022-12-07 23:49:47.835624 Epoch 237, Training Loss 0.899428188038604\n",
            "2022-12-07 23:50:01.317964 Epoch 238, Training Loss 0.8989635600763208\n",
            "2022-12-07 23:50:14.915617 Epoch 239, Training Loss 0.8978101598942066\n",
            "2022-12-07 23:50:28.347556 Epoch 240, Training Loss 0.8975052947126081\n",
            "2022-12-07 23:50:41.448914 Epoch 241, Training Loss 0.8977459734663025\n",
            "2022-12-07 23:50:54.469215 Epoch 242, Training Loss 0.897426895625756\n",
            "2022-12-07 23:51:07.581370 Epoch 243, Training Loss 0.8956600125031093\n",
            "2022-12-07 23:51:20.649258 Epoch 244, Training Loss 0.8987888782987814\n",
            "2022-12-07 23:51:33.608515 Epoch 245, Training Loss 0.8971382606669766\n",
            "2022-12-07 23:51:46.651937 Epoch 246, Training Loss 0.896220075885963\n",
            "2022-12-07 23:51:59.689362 Epoch 247, Training Loss 0.8973229802630441\n",
            "2022-12-07 23:52:12.791999 Epoch 248, Training Loss 0.8973700851583115\n",
            "2022-12-07 23:52:25.834755 Epoch 249, Training Loss 0.8976065460830698\n",
            "2022-12-07 23:52:38.934215 Epoch 250, Training Loss 0.8970006592285907\n",
            "2022-12-07 23:52:51.990847 Epoch 251, Training Loss 0.8963713959202437\n",
            "2022-12-07 23:53:05.077974 Epoch 252, Training Loss 0.8972406170099897\n",
            "2022-12-07 23:53:18.134354 Epoch 253, Training Loss 0.8973717664361305\n",
            "2022-12-07 23:53:31.275392 Epoch 254, Training Loss 0.8953913615640167\n",
            "2022-12-07 23:53:44.559271 Epoch 255, Training Loss 0.8962371653455603\n",
            "2022-12-07 23:53:57.697574 Epoch 256, Training Loss 0.8954894155492563\n",
            "2022-12-07 23:54:10.854501 Epoch 257, Training Loss 0.895134852213018\n",
            "2022-12-07 23:54:23.941552 Epoch 258, Training Loss 0.8956469209755168\n",
            "2022-12-07 23:54:36.999876 Epoch 259, Training Loss 0.8966003487177212\n",
            "2022-12-07 23:54:50.054725 Epoch 260, Training Loss 0.8955436339768608\n",
            "2022-12-07 23:55:03.215263 Epoch 261, Training Loss 0.8943146674529366\n",
            "2022-12-07 23:55:16.477291 Epoch 262, Training Loss 0.8955954446664551\n",
            "2022-12-07 23:55:29.581569 Epoch 263, Training Loss 0.8975682343211016\n",
            "2022-12-07 23:55:42.723450 Epoch 264, Training Loss 0.8963998643028767\n",
            "2022-12-07 23:55:55.793517 Epoch 265, Training Loss 0.8954931259002832\n",
            "2022-12-07 23:56:08.824649 Epoch 266, Training Loss 0.8953307379237221\n",
            "2022-12-07 23:56:22.010389 Epoch 267, Training Loss 0.8957275558463142\n",
            "2022-12-07 23:56:35.212602 Epoch 268, Training Loss 0.8946702342356563\n",
            "2022-12-07 23:56:48.500154 Epoch 269, Training Loss 0.8952502869736508\n",
            "2022-12-07 23:57:01.659843 Epoch 270, Training Loss 0.8965768752348088\n",
            "2022-12-07 23:57:14.647911 Epoch 271, Training Loss 0.894544683758865\n",
            "2022-12-07 23:57:27.761576 Epoch 272, Training Loss 0.8956320824677987\n",
            "2022-12-07 23:57:40.927502 Epoch 273, Training Loss 0.8932056270749368\n",
            "2022-12-07 23:57:54.300328 Epoch 274, Training Loss 0.8946846375989792\n",
            "2022-12-07 23:58:07.638653 Epoch 275, Training Loss 0.8953383603059423\n",
            "2022-12-07 23:58:20.736740 Epoch 276, Training Loss 0.8959972192259396\n",
            "2022-12-07 23:58:33.739541 Epoch 277, Training Loss 0.8942105526204609\n",
            "2022-12-07 23:58:46.827733 Epoch 278, Training Loss 0.8949132385613668\n",
            "2022-12-07 23:58:59.993951 Epoch 279, Training Loss 0.89459560060745\n",
            "2022-12-07 23:59:13.103670 Epoch 280, Training Loss 0.8933706532049057\n",
            "2022-12-07 23:59:26.250036 Epoch 281, Training Loss 0.8933801497035014\n",
            "2022-12-07 23:59:39.314991 Epoch 282, Training Loss 0.8939542285621623\n",
            "2022-12-07 23:59:52.549921 Epoch 283, Training Loss 0.8954189166693431\n",
            "2022-12-08 00:00:05.582246 Epoch 284, Training Loss 0.8936300813541997\n",
            "2022-12-08 00:00:18.593243 Epoch 285, Training Loss 0.8914449423780222\n",
            "2022-12-08 00:00:31.545949 Epoch 286, Training Loss 0.8946544322211419\n",
            "2022-12-08 00:00:44.611541 Epoch 287, Training Loss 0.8939576133742662\n",
            "2022-12-08 00:00:57.769811 Epoch 288, Training Loss 0.8936094243813049\n",
            "2022-12-08 00:01:10.793677 Epoch 289, Training Loss 0.8925777458778733\n",
            "2022-12-08 00:01:23.962527 Epoch 290, Training Loss 0.8936398331542759\n",
            "2022-12-08 00:01:36.951756 Epoch 291, Training Loss 0.8931877040649618\n",
            "2022-12-08 00:01:50.044628 Epoch 292, Training Loss 0.893537679672851\n",
            "2022-12-08 00:02:03.124922 Epoch 293, Training Loss 0.8911206716924067\n",
            "2022-12-08 00:02:16.270486 Epoch 294, Training Loss 0.8922966524310734\n",
            "2022-12-08 00:02:29.356180 Epoch 295, Training Loss 0.8925118557632427\n",
            "2022-12-08 00:02:42.433332 Epoch 296, Training Loss 0.8942153594073128\n",
            "2022-12-08 00:02:55.502276 Epoch 297, Training Loss 0.8914122246110531\n",
            "2022-12-08 00:03:08.580363 Epoch 298, Training Loss 0.8949312634022949\n",
            "2022-12-08 00:03:21.878862 Epoch 299, Training Loss 0.8924742852482954\n",
            "2022-12-08 00:03:35.097755 Epoch 300, Training Loss 0.8922775510479423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                          shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                        shuffle=False)\n",
        "validate(model, train_loader, val_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2W6_I4hDXKB",
        "outputId": "5316d8ea-0de8-45cc-88e6-fa0d051aef77"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.76\n",
            "Accuracy val: 0.68\n"
          ]
        }
      ]
    }
  ]
}